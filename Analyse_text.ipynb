{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2830c0d0207b43829e5edb31e4214386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a58d60f9934c46e2bf6c6ecc1912fb27",
              "IPY_MODEL_040d4b33cbec4599b0233999a09d4023",
              "IPY_MODEL_1d2facb24ee143c6a1f18eb8c14204c7"
            ],
            "layout": "IPY_MODEL_12da2c9205894cf4b6bc9b1b34828913"
          }
        },
        "a58d60f9934c46e2bf6c6ecc1912fb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1ed83aa4737412d952928c0f4acbcc9",
            "placeholder": "​",
            "style": "IPY_MODEL_c5bc37521bc84b3c98e9335ee320666b",
            "value": "config.json: 100%"
          }
        },
        "040d4b33cbec4599b0233999a09d4023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef7fdc9433246679eef2f3e97c6ffce",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efad2f30eb024498be344ad8b251e957",
            "value": 629
          }
        },
        "1d2facb24ee143c6a1f18eb8c14204c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270996a1243a431cb8bc1f475a940ab6",
            "placeholder": "​",
            "style": "IPY_MODEL_a0744227e6b44eb0a582aac7fe3dbc88",
            "value": " 629/629 [00:00&lt;00:00, 40.4kB/s]"
          }
        },
        "12da2c9205894cf4b6bc9b1b34828913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1ed83aa4737412d952928c0f4acbcc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5bc37521bc84b3c98e9335ee320666b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef7fdc9433246679eef2f3e97c6ffce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efad2f30eb024498be344ad8b251e957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "270996a1243a431cb8bc1f475a940ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0744227e6b44eb0a582aac7fe3dbc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14b405539fa341dd991829f1427ad879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fb517053ebf4473b77270ae53a12dfb",
              "IPY_MODEL_cbee7f34885243dfb67dbabd674de70f",
              "IPY_MODEL_9bd820f754b84210a4e70301b1de2edd"
            ],
            "layout": "IPY_MODEL_4b3a7b6520c641fb944e707e60f75f1b"
          }
        },
        "6fb517053ebf4473b77270ae53a12dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa5919cf542241f5b2b2ebaa5afece78",
            "placeholder": "​",
            "style": "IPY_MODEL_503a448c81f342b6b235d70f9c4a2f4c",
            "value": "model.safetensors: 100%"
          }
        },
        "cbee7f34885243dfb67dbabd674de70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0ba0825df44d61ad5ac2bb19ddc1a6",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abdb76ef8814cc696e26b6215bcaf91",
            "value": 267832558
          }
        },
        "9bd820f754b84210a4e70301b1de2edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ca07039c9014068b64f6dc721b2db39",
            "placeholder": "​",
            "style": "IPY_MODEL_2da309823f25480c927b3d71c0c7e090",
            "value": " 268M/268M [00:02&lt;00:00, 158MB/s]"
          }
        },
        "4b3a7b6520c641fb944e707e60f75f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5919cf542241f5b2b2ebaa5afece78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503a448c81f342b6b235d70f9c4a2f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a0ba0825df44d61ad5ac2bb19ddc1a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abdb76ef8814cc696e26b6215bcaf91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ca07039c9014068b64f6dc721b2db39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da309823f25480c927b3d71c0c7e090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc980a6b17484b4b8970a79b7d62e467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e636e0be67d42b1969f08549be0c669",
              "IPY_MODEL_0acf199117e64be5b1c9f5e4c0b35408",
              "IPY_MODEL_3d1505768d2647028eb1e27b9e1204bf"
            ],
            "layout": "IPY_MODEL_5d0ba503b0bc4b168efbb252774ace36"
          }
        },
        "4e636e0be67d42b1969f08549be0c669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4680952c5b84d8f8ab233f7caae012a",
            "placeholder": "​",
            "style": "IPY_MODEL_c0a171df34eb4ea2a243154a5b6d7b90",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0acf199117e64be5b1c9f5e4c0b35408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8a6885adb4b42f28c57e9ee677fcd4e",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df6e26c5da3a484ea109eb6b3078f9e8",
            "value": 48
          }
        },
        "3d1505768d2647028eb1e27b9e1204bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e4d1a7ceda4526b3022c916441d89c",
            "placeholder": "​",
            "style": "IPY_MODEL_3146b6474a774beb9bc3952ddd968f71",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.83kB/s]"
          }
        },
        "5d0ba503b0bc4b168efbb252774ace36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4680952c5b84d8f8ab233f7caae012a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a171df34eb4ea2a243154a5b6d7b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8a6885adb4b42f28c57e9ee677fcd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6e26c5da3a484ea109eb6b3078f9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55e4d1a7ceda4526b3022c916441d89c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3146b6474a774beb9bc3952ddd968f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "775f2a963f0f4f9687e3eec1495e4103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac70b3c74a114a539b4bdc6ebcb29a13",
              "IPY_MODEL_e122de7f1ed74404b595b33aff39ec0e",
              "IPY_MODEL_1f4d1237ffa64ada801d0699c8a6f773"
            ],
            "layout": "IPY_MODEL_a689bf1a1987405199a66cf999d9b082"
          }
        },
        "ac70b3c74a114a539b4bdc6ebcb29a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2181f636fd6b4a4b817b267a9224d995",
            "placeholder": "​",
            "style": "IPY_MODEL_b85d3996b14d4a63af4f1218bdf306fc",
            "value": "vocab.txt: 100%"
          }
        },
        "e122de7f1ed74404b595b33aff39ec0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a1c8720072493fa4ccab5fca92e3ea",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05c493ecbb074a7aa8a43089fcfc8089",
            "value": 231508
          }
        },
        "1f4d1237ffa64ada801d0699c8a6f773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f999430c6a42e4b895f7d3a486f82c",
            "placeholder": "​",
            "style": "IPY_MODEL_54bd7129978746b1ae80f0749f9a9155",
            "value": " 232k/232k [00:00&lt;00:00, 3.43MB/s]"
          }
        },
        "a689bf1a1987405199a66cf999d9b082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2181f636fd6b4a4b817b267a9224d995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b85d3996b14d4a63af4f1218bdf306fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94a1c8720072493fa4ccab5fca92e3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c493ecbb074a7aa8a43089fcfc8089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72f999430c6a42e4b895f7d3a486f82c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54bd7129978746b1ae80f0749f9a9155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# L'analyse du roman The Hound of the Baskervilles"
      ],
      "metadata": {
        "id": "TAul6-uf2-kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Telechargement/Nettoyage l'en-tête et le pied de texte"
      ],
      "metadata": {
        "id": "8KxLU1uW3S7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL du livre \"The Hound of the Baskervilles\" sur Project Gutenberg\n",
        "url = 'https://www.gutenberg.org/cache/epub/2852/pg2852.txt'\n",
        "\n",
        "# Télécharger le contenu du fichier\n",
        "response = requests.get(url)\n",
        "\n",
        "# Lire le texte du livre\n",
        "text = response.text\n",
        "\n",
        "# Afficher les 1000 premiers caractères\n",
        "print(text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf95MHq-imCK",
        "outputId": "fb539e71-041c-47b4-a341-4b516841573c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The Hound of the Baskervilles\r\n",
            "    \r\n",
            "This ebook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this ebook or online\r\n",
            "at www.gutenberg.org. If you are not located in the United States,\r\n",
            "you will have to check the laws of the country where you are located\r\n",
            "before using this eBook.\r\n",
            "\r\n",
            "Title: The Hound of the Baskervilles\r\n",
            "\r\n",
            "Author: Arthur Conan Doyle\r\n",
            "\r\n",
            "Release date: October 1, 2001 [eBook #2852]\r\n",
            "                Most recently updated: June 27, 2021\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "Credits: Shreevatsa R, and David Widger\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK THE HOUND OF THE BASKERVILLES ***\r\n",
            "\r\n",
            "cover \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THE HOUND OF THE BASKERVILLES\r\n",
            "\r\n",
            "Another Adventure of Sherlock Holmes\r\n",
            "\r\n",
            "\r\n",
            "by A. Conan Doyle\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "My dear Robinson,\r\n",
            "\r\n",
            "\r\n",
            "    It was to your accoun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gutenberg_text(text):\n",
        "    lines = text.split(\"\\n\")  # Diviser le texte en lignes\n",
        "\n",
        "    # Trouver les indices des lignes marquant le début et la fin du texte\n",
        "    start_idx = 0\n",
        "    end_idx = len(lines)\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"START OF THE PROJECT GUTENBERG EBOOK THE HOUND OF THE BASKERVILLES\" in line:\n",
        "            start_idx = i + 1  # Commencer après cette ligne\n",
        "        elif \"END OF THE PROJECT GUTENBERG EBOOK THE HOUND OF THE BASKERVILLES\" in line:\n",
        "            end_idx = i\n",
        "            break  # Arrêter dès qu'on trouve la fin\n",
        "\n",
        "    # Extraire uniquement le texte du livre\n",
        "    cleaned_text = \"\\n\".join(lines[start_idx:end_idx]).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "# Nettoyer le texte\n",
        "cleaned_text = clean_gutenberg_text(text)\n",
        "\n",
        "# Afficher un extrait nettoyé\n",
        "print(cleaned_text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDNcisG6mVjX",
        "outputId": "d93fa4e1-87d3-49f4-d9ac-c925c2255c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cover \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THE HOUND OF THE BASKERVILLES\r\n",
            "\r\n",
            "Another Adventure of Sherlock Holmes\r\n",
            "\r\n",
            "\r\n",
            "by A. Conan Doyle\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "My dear Robinson,\r\n",
            "\r\n",
            "\r\n",
            "    It was to your account of a West-Country legend that this tale owes its\r\n",
            "inception. For this and for your help in the details all thanks.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Yours most truly,        \r\n",
            "\r\n",
            "A. Conan Doyle.    \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Hindhead,\r\n",
            "\r\n",
            "    Haslemere.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            "\r\n",
            " Chapter 1  Mr. Sherlock Holmes\r\n",
            " Chapter 2  The Curse of the Baskervilles\r\n",
            " Chapter 3  The Problem\r\n",
            " Chapter 4  Sir Henry Baskerville\r\n",
            " Chapter 5  Three Broken Threads\r\n",
            " Chapter 6  Baskerville Hall\r\n",
            " Chapter 7  The Stapletons of Merripit House\r\n",
            " Chapter 8  First Report of Dr. Watson\r\n",
            " Chapter 9  The Light upon the Moor [Second Report of Dr. Watson]\r\n",
            " Chapter 10 Extract from the Diary of Dr. Watson\r\n",
            " Chapter 11 The Man on the Tor\r\n",
            " Chapter 12 Death on the Moor\r\n",
            " Chapter 13 Fixing the Nets\r\n",
            " Chapter 14 The Hound of the Baskervilles\r\n",
            " Chapter 15 A Retrospection\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Chapter 1.\r\n",
            "Mr. Sherlock Holmes\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_text[-1000:]) # afficher les 1000 dernieres"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDRAd78BnABs",
        "outputId": "fada288c-445b-4a3b-f1f5-ce3e6b3c0263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "al occasions. There were\r\n",
            "      three possible courses. He might claim the property from South\r\n",
            "      America, establish his identity before the British authorities\r\n",
            "      there and so obtain the fortune without ever coming to England at\r\n",
            "      all, or he might adopt an elaborate disguise during the short\r\n",
            "      time that he need be in London; or, again, he might furnish an\r\n",
            "      accomplice with the proofs and papers, putting him in as heir,\r\n",
            "      and retaining a claim upon some proportion of his income. We\r\n",
            "      cannot doubt from what we know of him that he would have found\r\n",
            "      some way out of the difficulty. And now, my dear Watson, we have\r\n",
            "      had some weeks of severe work, and for one evening, I think, we\r\n",
            "      may turn our thoughts into more pleasant channels. I have a box\r\n",
            "      for _Les Huguenots_. Have you heard the De Reszkes? Might I\r\n",
            "      trouble you then to be ready in half an hour, and we can stop at\r\n",
            "      Marcini’s for a little dinner on the way?”\r\n",
            "\r\n",
            "\r\n",
            "THE END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_text[:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyI86n7mrYwc",
        "outputId": "a37107e1-9982-4517-fd72-3b31ba013819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cover \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THE HOUND OF THE BASKERVILLES\r\n",
            "\r\n",
            "Another Adventure of Sherlock Holmes\r\n",
            "\r\n",
            "\r\n",
            "by A. Conan Doyle\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "My dear Robinson,\r\n",
            "\r\n",
            "\r\n",
            "    It was to your account of a West-Country legend that this tale owes its\r\n",
            "inception. For this and for your help in the details all thanks.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Yours most truly,        \r\n",
            "\r\n",
            "A. Conan Doyle.    \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Hindhead,\r\n",
            "\r\n",
            "    Haslemere.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            "\r\n",
            " Chapter 1  Mr. Sherlock Holmes\r\n",
            " Chapter 2  The Curse of the Baskervilles\r\n",
            " Chapter 3  The Problem\r\n",
            " Chapter 4  Sir Henry Baskerville\r\n",
            " Chapter 5  Three Broken Threads\r\n",
            " Chapter 6  Baskerville Hall\r\n",
            " Chapter 7  The Stapletons of Merripit House\r\n",
            " Chapter 8  First Report of Dr. Watson\r\n",
            " Chapter 9  The Light upon the Moor [Second Report of Dr. Watson]\r\n",
            " Chapter 10 Extract from the Diary of Dr. Watson\r\n",
            " Chapter 11 The Man on the Tor\r\n",
            " Chapter 12 Death on the Moor\r\n",
            " Chapter 13 Fixing the Nets\r\n",
            " Chapter 14 The Hound of the Baskervilles\r\n",
            " Chapter 15 A Retrospection\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Chapter 1.\r\n",
            "Mr. Sherlock Holmes\r\n",
            "\r\n",
            "\r\n",
            "      Mr. Sherlock Holmes, who was usually very late in the mornings,\r\n",
            "      save upon those not infrequent occasions when he was up all\r\n",
            "      night, was seated at the breakfast table. I stood upon the\r\n",
            "      hearth-rug and picked up the stick which our visitor had left\r\n",
            "      behind him the night before. It was a fine, thick piece of wood,\r\n",
            "      bulbous-headed, of the sort which is known as a “Penang lawyer.”\r\n",
            "      Just under the head was a broad silver band nearly an inch\r\n",
            "      across. “To James Mortimer, M.R.C.S., from his friends of the\r\n",
            "      C.C.H.,” was engraved upon it, with the date “1884.” It was just\r\n",
            "      such a stick as the old-fashioned family practitioner used to\r\n",
            "      carry—dignified, solid, and reassuring.\r\n",
            "\r\n",
            "      “Well, Watson, what do you make of it?”\r\n",
            "\r\n",
            "      Holmes was sitting with his back to me, and I had given him no\r\n",
            "      sign of my occupation.\r\n",
            "\r\n",
            "      “How did you know what I was doing? I believe you have eyes in\r\n",
            "      the back of your head.”\r\n",
            "\r\n",
            "      “I have, at least, a well-polished, silver-plated coffee-pot in\r\n",
            "      front of me,” said he. “But, tell me, Watson, what do you make of\r\n",
            "      our visitor’s stick? Since we have been so unfortunate as to miss\r\n",
            "      him and have no notion of his errand, this accidental souvenir\r\n",
            "      becomes of importance. Let me hear you reconstruct the man by an\r\n",
            "      examination of it.”\r\n",
            "\r\n",
            "      “I think,” said I, following as far as I could the methods of my\r\n",
            "      companion, “that Dr. Mortimer is a successful, elderly medical\r\n",
            "      man, well-esteemed since those who know him give him this mark of\r\n",
            "      their appreciation.”\r\n",
            "\r\n",
            "      “Good!” said Holmes. “Excellent!”\r\n",
            "\r\n",
            "      “I think also that the probability is in favour of his being a\r\n",
            "      country practitioner who does a great deal of his visiting on\r\n",
            "      foot.”\r\n",
            "\r\n",
            "      “Why so?”\r\n",
            "\r\n",
            "      “Because this stick, though originally a very handsome one has\r\n",
            "      been so knocked about that I can hardly imagine a town\r\n",
            "      practitione\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wyY-HnvuF9x",
        "outputId": "bce2dfa0-6598-4fb6-cf37-4e90e732eccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisation en mots\n",
        "words = word_tokenize(cleaned_text.lower())\n",
        "print(words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWerC119uMMp",
        "outputId": "80d8792a-f337-4d5e-977a-ad7dcab10b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cover', 'the', 'hound', 'of', 'the', 'baskervilles', 'another', 'adventure', 'of', 'sherlock', 'holmes', 'by', 'a.', 'conan', 'doyle', 'my', 'dear', 'robinson', ',', 'it', 'was', 'to', 'your', 'account', 'of', 'a', 'west-country', 'legend', 'that', 'this', 'tale', 'owes', 'its', 'inception', '.', 'for', 'this', 'and', 'for', 'your', 'help', 'in', 'the', 'details', 'all', 'thanks', '.', 'yours', 'most', 'truly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisation en phrases\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "print(sentences[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aefjEkJwub0D",
        "outputId": "cd0338a1-13be-4f4d-87b5-180dfdeb2e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cover \\r\\n\\r\\n\\r\\n\\r\\nTHE HOUND OF THE BASKERVILLES\\r\\n\\r\\nAnother Adventure of Sherlock Holmes\\r\\n\\r\\n\\r\\nby A. Conan Doyle\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMy dear Robinson,\\r\\n\\r\\n\\r\\n    It was to your account of a West-Country legend that this tale owes its\\r\\ninception.', 'For this and for your help in the details all thanks.', 'Yours most truly,        \\r\\n\\r\\nA. Conan Doyle.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Les analyses de base\n"
      ],
      "metadata": {
        "id": "mXwlmxmzo0vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HRCYR5shnLFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text1 = nltk.Text(words)\n",
        "cleaned_text1.concordance(\"hound\") #Affiche les occurences de mots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv99eGCdp2W8",
        "outputId": "ab721fdb-4c2d-40e4-d1f3-f6f9ef96894c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 25 of 67 matches:\n",
            "                           cover the hound of the baskervilles another adventur\n",
            "er 13 fixing the nets chapter 14 the hound of the baskervilles chapter 15 a ret\n",
            "d narrative : “ of the origin of the hound of the baskervilles there have been \n",
            "and there ran mute behind him such a hound of hell as god forbid should ever be\n",
            " great , black beast , shaped like a hound , yet larger than any hound that eve\n",
            "d like a hound , yet larger than any hound that ever mortal eye has rested upon\n",
            "ale , my sons , of the coming of the hound which is said to have plagued the fa\n",
            "ge creature or heard the baying of a hound . the latter question he put to me s\n",
            "ey were the footprints of a gigantic hound ! ” chapter 3. the problem i confess\n",
            "otmark is material. ” “ the original hound was material enough to tug a man ’ s\n",
            " . “ of course , i ’ ve heard of the hound ever since i was in the nursery . it\n",
            " it ? ” “ the peasants say it is the hound of the baskervilles calling for its \n",
            "nger ? ” “ you know the story of the hound ? ” “ i do not believe in such nonse\n",
            "heep-dog of the moor ? or a spectral hound , black , silent , and monstrous ? w\n",
            "on . “ they say it is the cry of the hound of the baskervilles. ” he groaned an\n",
            "d was silent for a few moments . “ a hound it was , ” he said at last , “ but i\n",
            "nk yourself that it was the cry of a hound ? i am not a child . you need not fe\n",
            "strange bird. ” “ no , no , it was a hound . my god , can there be some truth i\n",
            "cle ! there was the footprint of the hound beside him as he lay . it all fits t\n",
            "ch resembled the distant baying of a hound . it is incredible , impossible , th\n",
            "ordinary laws of nature . a spectral hound which leaves material footmarks and \n",
            "ose that there were really some huge hound loose upon it ; that would go far to\n",
            " everything . but where could such a hound lie concealed , where did it get its\n",
            " other . and always , apart from the hound , there is the fact of the human age\n",
            ", constant murmur of the sea . “ the hound ! ” cried holmes . “ come , watson ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text1.similar(\"hound\", num=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJlMQVO2s6Vk",
        "outputId": "f1c35afb-2ee5-4920-afef-ab6d7fb40511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moor man case matter baronet letter place way darkness baskervilles\n",
            "hall light name time facts door path night stick paper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text1.common_contexts([\"hound\", \"dog\"]) #identifier les contextes communes des deux"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P21ar-76GNzh",
        "outputId": "b9664cc8-9890-41ad-e355-8d61ae0bdf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_has a_does\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text1.dispersion_plot([\"Holmes\", \"Watson\", \"Baskerville\", \"hound\", \"moor\"])#Tracer un dispersion plot de mots-clés"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OpVnKnDNGeRK",
        "outputId": "047e6806-6225-4e1b-aa1b-727914e22551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = FreqDist(cleaned_text1)#Affiche un graphique cumulatif des 50 mots les plus fréquents\n",
        "fdist.plot(50, cumulative=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "GXeaQKafG2oj",
        "outputId": "eb4cd57c-acd6-4b80-ec64-8d8666bea608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Samples', ylabel='Cumulative Counts'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_words = [w for w in set(cleaned_text1) if len(w) > 12]#Trouve et affiche les mots longs du roman.\n",
        "print(sorted(long_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QFrcM37HMju",
        "outputId": "fac4bc94-d2d6-4a00-b66b-7751bf75888a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_tête-à-tête_', 'absent-minded', 'acquaintances', 'afterthoughts', 'age-blackened', 'anguish—burst', 'anthropological', 'appreciation.', 'autobiography', 'banqueting-hall', 'baskervilles.', 'baskerville—that', 'billiard-room', 'bird—practically', 'black-bearded', 'black-clothed', 'blackmailing.', 'boulder-sprinkled', 'brandy-bottle', 'breast-pocket', 'breathlessness', 'brother-in-law', 'bulbous-headed', 'butterfly-net', 'carry—dignified', 'characteristic', 'characteristics', 'circumspectly', 'circumstances', 'communication', 'companionship', 'comparatively', 'concentration', 'conscientious', 'consideration', 'conversations', 'corresponding', 'counteracting', 'cross-examined', 'cross-questioned', 'cruel-hearted', 'different—had', 'disadvantage.', 'disappearance', 'disappointment', 'disapprobation', 'distinguished', 'distortion—so', 'dolichocephalic', 'dressing-gown', 'driving-wheel', 'extraordinarily', 'extraordinary', 'flaxen-haired', 'good-morning.', 'granite-flecked', 'green-scummed', 'green-splotched', 'grey-whiskered', 'half-sovereign', 'heavy-featured', 'horror-struck', 'house-physician—little', 'house-surgeon', 'hunting-crop.', 'identification', 'ill-treatment—that', 'important—the', 'inadvertently—', 'inconceivable', 'incredulously', 'incriminating', 'indescribably', 'insignificant', 'interruption—and', 'introductions', 'investigating', 'investigation', 'investigation.', 'investigations', 'irretrievably', 'justification', 'long-standing', 'meek-mannered', 'miscalculation', 'nail-scissors', 'naturalist—stapleton', 'neighbourhood', 'northumberland', 'nothing—nothing', 'old-fashioned', 'opportunities', 'personification', 'preoccupations', 'probabilities', 'probability—and', 'professionals', 'questioning—stared', 'railway-carriage', 'reading—frankland', 'recollections', 'reconstruction', 'reincarnation', 'remarkable-looking', 'representative', 'responsibility', 'retrospection', 'rough-looking', 'school-mastering', 'scientifically', 'scullery-maid', 'self-contained', 'self-importance', 'self-respect—everything', 'shamefully—shamefully', 'silver-plated', 'silver-spangled', 'silver-tipped', 'single-minded', 'slate-coloured', 'smoke-darkened', 'solicitations', 'spanish-american', 'spread-eagled', 'station-master', 'step-daughter', 'striking-looking', 'strong-minded', 'suggestiveness', 'suggests—halloa', 'supernatural.', 'supernaturalists', 'superstitious', 'supra-orbital', 'suspicions—not', 'telegraph-office', 'three-quarters', 'uncomfortably', 'unconcernedly', 'uncontrollable', 'understanding', 'unfortunately', 'unimaginative', 'uninteresting', 'unjustifiable', 'vegetation—sad', 'visiting-card', 'watson—refined', 'watson—running', 'weather-beaten', 'weather-bitten', 'well-established', 'well-esteemed', 'well-polished', 'westmoreland.', 'wide-stretching']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted([w for w in set(cleaned_text1) if w.endswith(\"ness\")]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXfwxs5HHYJG",
        "outputId": "933d2ea7-ae6b-4961-8f23-434b12a508c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alertness', 'bitterness', 'blackness', 'breathlessness', 'business', 'carelessness', 'cleanliness', 'cleverness', 'coarseness', 'darkness', 'directness', 'eagerness', 'earnestness', 'frankness', 'freshness', 'goodness', 'happiness', 'hardness', 'illness', 'kindness', 'lioness', 'loneliness', 'looseness', 'promptness', 'quickness', 'readiness', 'rudeness', 'sadness', 'selfishness', 'stillness', 'suggestiveness', 'thankfulness', 'uneasiness', 'vagueness', 'vastness', 'wilderness', 'witness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted([w for w in set(cleaned_text1) if w.startswith(\"hol\")]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh6MW-FvHnhf",
        "outputId": "a72cee89-13f9-4ef9-f09b-1476aa738545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hold', 'holding', 'holes', 'hollow', 'holmes', 'holmes.', 'holy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_list = list(nltk.bigrams(cleaned_text1))#Crée des paires de mots consécutifs\n",
        "print(bigrams_list[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBcKsc2gHy3z",
        "outputId": "c50a93e3-d4a7-4c1e-80a5-dd09ada13576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cover', 'the'), ('the', 'hound'), ('hound', 'of'), ('of', 'the'), ('the', 'baskervilles'), ('baskervilles', 'another'), ('another', 'adventure'), ('adventure', 'of'), ('of', 'sherlock'), ('sherlock', 'holmes'), ('holmes', 'by'), ('by', 'a.'), ('a.', 'conan'), ('conan', 'doyle'), ('doyle', 'my'), ('my', 'dear'), ('dear', 'robinson'), ('robinson', ','), (',', 'it'), ('it', 'was')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg, names, cmudict\n",
        "from nltk.probability import ConditionalFreqDist, FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CQwnsNlIMIO",
        "outputId": "3a23cf41-1a0e-4e25-b5b4-43b2cf8e2ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('cmudict')\n",
        "nltk.download('names')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnXlIhhbMp6_",
        "outputId": "a41a7725-1412-4a8d-8af7-f110d8d3ce46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sentence = max(sentences, key=len)\n",
        "print(\"\\n La plus longue phrase du roman :\")\n",
        "print(\" \".join(longest_sentence))\n",
        "print(f\"\\n Nombre de mots : {len(longest_sentence)}\")"
      ],
      "metadata": {
        "id": "XiMdiemLKKlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstituer les 3 premières phrases\n",
        "print(\"\\n 3 premières phrases reconstituées :\")\n",
        "for sent in sentences[:3]:\n",
        "    print(\" \".join(sent))"
      ],
      "metadata": {
        "id": "PvgsOudcIyQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse des frequences\n",
        "cfd = ConditionalFreqDist()\n",
        "\n",
        "for word in words:\n",
        "    cfd[len(word)][word.lower()] += 1  # Comptage en minuscule pour éviter la casse\n",
        "\n",
        "print(\"\\n 10 mots les plus fréquents de 10 lettres :\")\n",
        "print(cfd[10].most_common(10))"
      ],
      "metadata": {
        "id": "zJWPokggLdBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse de la prononciation\n",
        "pron_dict = cmudict.dict()\n",
        "\n",
        "def get_pronunciation(word):\n",
        "    \"\"\"Renvoie la prononciation du mot s'il existe dans cmudict\"\"\"\n",
        "    word = word.lower()\n",
        "    if word in pron_dict:\n",
        "        return pron_dict[word]\n",
        "    else:\n",
        "        return [\" Prononciation inconnue\"]\n",
        "\n",
        "# Tester quelques mots du roman\n",
        "words_to_test = [\"hound\", \"Baskerville\", \"moor\", \"Holmes\", \"Watson\"]\n",
        "print(\"\\n Prononciation des mots clés :\")\n",
        "for word in words_to_test:\n",
        "    print(f\"{word} → {get_pronunciation(word)}\")\n"
      ],
      "metadata": {
        "id": "IXs9VdjBLusm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "skk0YjQNicHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir en UTF-8\n",
        "text_utf = cleaned_text.encode('utf-8').decode('utf-8')\n",
        "\n",
        "print(\"500 premiers caractères du roman :\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "EFTE2x1hMtJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens_utf = word_tokenize(text_utf)\n",
        "\n",
        "# Création d'un objet Text pour analyses\n",
        "text_nltk = Text(tokens_utf)\n",
        "\n",
        "print(\"\\n 20 premiers tokens :\")\n",
        "print(tokens_utf[:20])"
      ],
      "metadata": {
        "id": "5D8XdiQBhZk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtrer la ponctuation\n",
        "filtered_tokens = [w.lower() for w in tokens_utf if w.isalpha()]\n",
        "\n",
        "print(\"\\n 20 premiers tokens sans ponctuation :\")\n",
        "print(filtered_tokens[:20])\n",
        "print(f\"\\nNombre de tokens sans ponctuation : {len(filtered_tokens)}\")"
      ],
      "metadata": {
        "id": "EJWKuDk9jfMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution des lettres\n",
        "fdist_letters = FreqDist(\"\".join(filtered_tokens))\n",
        "\n",
        "# Afficher la fréquence des lettres\n",
        "plt.figure(figsize=(10, 4))\n",
        "fdist_letters.plot(26, cumulative=False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "8_4G8e_MjfGg",
        "outputId": "e6cb55a1-7f4c-4bd3-f9a3-91b11564e225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#expresions regulieres avec re.search()\n",
        "# Mots finissant par \"ed\"\n",
        "wordlist_ed = [w for w in filtered_tokens if re.search(r'ed$', w)]\n",
        "print(\"\\n Mots finissant par 'ed' :\", wordlist_ed[:10])\n",
        "\n",
        "# Mots contenant 'j' et 't' à des positions spécifiques\n",
        "wordlist_j_t_1 = [w for w in filtered_tokens if re.search(r'^..j..t..$', w)]\n",
        "print(\"\\n Mots correspondant au motif '..j..t..' :\", wordlist_j_t_1[:10])\n",
        "\n",
        "# Mots correspondant au motif `[ghi][mno][jlk][def]`\n",
        "Key_4653 = [w for w in filtered_tokens if re.search(r'^[ghi][mno][jlk][def]$', w)]\n",
        "print(\"\\n Mots correspondant à la structure '[ghi][mno][jlk][def]' :\", Key_4653)\n",
        "\n",
        "# Mots contenant uniquement des lettres de g à o\n",
        "Key_456_bis = [w for w in filtered_tokens if re.search(r'^[g-o]+$', w)]\n",
        "print(\"\\n Mots avec uniquement des lettres de g à o :\", Key_456_bis[:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "u6T497uilxrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse morphologique\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Stemming\n",
        "print(\"\\n Stemming avec PorterStemmer :\")\n",
        "print([porter.stem(t) for t in filtered_tokens])\n",
        "print(\"\\n Stemming avec LancasterStemmer :\")\n",
        "print([lancaster.stem(t) for t in filtered_tokens])\n"
      ],
      "metadata": {
        "id": "MyUA4LsZmt2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"happiness\", \"flies\", \"died\", \"jumping\"]\n",
        "\n",
        "print(\"\\n Stemming avec PorterStemmer :\")\n",
        "print([porter.stem(w) for w in words])\n",
        "\n",
        "print(\"\\n Stemming avec LancasterStemmer :\")\n",
        "print([lancaster.stem(w) for w in words])"
      ],
      "metadata": {
        "id": "L1TlaDA9nTqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "38qQep7wom_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trouver des mots avec \"a\" suivi de n'importe quel mot suivi de \"man\"\n",
        "print(\"\\n Mots entre 'a' et 'man' :\")\n",
        "print(text_nltk .findall(r\"<a> (<.*>) <man>\"))\n",
        "\n",
        "# Trouver des hobbies et intérêts\n",
        "print(\"\\n Structure '<mot> and other <mot>s' :\")\n",
        "print(text_nltk .findall(r\"<\\w*> <and> <other> <\\w*s>\"))"
      ],
      "metadata": {
        "id": "b5HFxfUbnoqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mots_longs(words, min_len=12):\n",
        "    for w in words:\n",
        "        if len(w) >= min_len:\n",
        "            yield w\n",
        "\n",
        "def bigrammes_genere(words):\n",
        "    for w1, w2 in zip(words, words[1:]):\n",
        "        yield (w1, w2)\n",
        "\n",
        "\n",
        "\n",
        "print(\" Mots longs (>=12 lettres) :\")\n",
        "for mot in mots_longs(filtered_tokens):\n",
        "    print(mot)\n",
        "\n",
        "print(\"\\n Quelques bigrammes :\")\n",
        "for i, bg in enumerate(bigrammes_genere(filtered_tokens)):\n",
        "    print(bg)\n",
        "    if i > 10: break\n"
      ],
      "metadata": {
        "id": "wG84lh1wprxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_ch2 = requests.get(url).text\n",
        "\n",
        "# Extraire le chapitre 2\n",
        "start = raw_text_ch2.find(\"Chapter 2.\")\n",
        "end = raw_text_ch2.find(\"Chapter 3.\")\n",
        "chapter2 = raw_text_ch2[start:end]\n",
        "\n",
        "\n",
        "print(chapter2[-500:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlupAll0o5Vs",
        "outputId": "cf99f227-cb53-4d6f-917a-f70d977ae834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "se statement was made by Barrymore at the\r\n",
            "      inquest. He said that there were no traces upon the ground round\r\n",
            "      the body. He did not observe any. But I did—some little distance\r\n",
            "      off, but fresh and clear.”\r\n",
            "\r\n",
            "      “Footprints?”\r\n",
            "\r\n",
            "      “Footprints.”\r\n",
            "\r\n",
            "      “A man’s or a woman’s?”\r\n",
            "\r\n",
            "      Dr. Mortimer looked strangely at us for an instant, and his voice\r\n",
            "      sank almost to a whisper as he answered.\r\n",
            "\r\n",
            "      “Mr. Holmes, they were the footprints of a gigantic hound!”\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, RegexpParser\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fO-DScap05r",
        "outputId": "0ddbf611-76a1-4b4b-99d3-939e12d6b57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Fonction utilitaire pour la POS tag conversion\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Traitement par phrases\n",
        "sentences_chap2 = sent_tokenize(chapter2)\n",
        "\n",
        "for sent in sentences_chap2[:5]:\n",
        "    tokens = word_tokenize(sent)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatisation\n",
        "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged]\n",
        "    print(\"Lemmatisé :\", \" \".join(lemmatized))\n",
        "    # Chunking pour groupes nominaux\n",
        "    chunk_grammar = r\"NP: {<DT>?<JJ.*>*<NN.*>}\"\n",
        "    chunk_parser = RegexpParser(chunk_grammar)\n",
        "    chunked = chunk_parser.parse(tagged)\n",
        "\n",
        "    print(\" Groupes nominaux :\")\n",
        "    for subtree in chunked.subtrees(lambda t: t.label() == 'NP'):\n",
        "        print(\" →\", \" \".join(word for word, tag in subtree.leaves()))"
      ],
      "metadata": {
        "id": "BT9jyGt4p2IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code effectue une analyse de texte en deux étapes principales : la lemmatisation des mots et l'extraction des groupes nominaux. La lemmatisation réduit les mots à leur forme de base, tandis que le chunking identifie les groupes nominaux dans les phrases. Les résultats montrent que les groupes nominaux sont bien extraits, bien que certains éléments de ponctuation ou de mots non pertinents apparaissent."
      ],
      "metadata": {
        "id": "G3HQHm-q6Z3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synonymes"
      ],
      "metadata": {
        "id": "zBgIiUTHyLVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Mot dont on veut récupérer les synonymes\n",
        "word = \"retrospection\"\n",
        "# Construire l'URL\n",
        "url = f\"https://www.synonyms.com/synonym/{word}\"\n",
        "# Ajouter un User-Agent pour simuler un navigateur\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "# Envoyer une requête GET\n",
        "response = requests.get(url, headers=headers)\n",
        "# Vérifier si la requête a réussi\n",
        "if response.status_code == 200:\n",
        "    # Parser le contenu HTML\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Récupérer tous les termes synonymes\n",
        "    synonyms = [item.text.strip() for item in soup.find_all(class_=\"syns\")]\n",
        "    # Afficher les synonymes\n",
        "    print(\"Synonymes trouvés :\")\n",
        "    for syn in synonyms:\n",
        "        print(f\"- {syn}\")\n",
        "else:\n",
        "    print(f\"Erreur lors de la récupération des données (statut: {response.status_code})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztq3eiYMyOB6",
        "outputId": "768f357d-6525-4160-ffd3-d18825f42cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonymes trouvés :\n",
            "- Synonyms:memory, recollection, remembrance, reminiscence, retrospect\n",
            "- Synonyms:looking back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syns=set()\n",
        "for s in synonyms:\n",
        "  if s.startswith(\"Synonyms:\"):\n",
        "    l=s.split(\":\")[1].split(\",\")\n",
        "    for item in l:\n",
        "      syns.add(item.strip())"
      ],
      "metadata": {
        "id": "8xzzouX2y9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDSR-b2izEYF",
        "outputId": "d4d131f5-6334-42f6-820a-96a5306a9165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'looking back',\n",
              " 'memory',\n",
              " 'recollection',\n",
              " 'remembrance',\n",
              " 'reminiscence',\n",
              " 'retrospect'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changement le style du texte extrait"
      ],
      "metadata": {
        "id": "yZPamMJQ0TU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jktDhf-J0z9e",
        "outputId": "c5518524-9a80-4e1e-8dc1-e9c71025964f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_ch6 = requests.get(url).text\n",
        "\n",
        "# Extraire le chapitre 2\n",
        "start = raw_text_ch6.find(\"Chapter 6.\")\n",
        "end = raw_text_ch6.find(\"Chapter 7.\")\n",
        "chapter6 = raw_text_ch6[start:end]"
      ],
      "metadata": {
        "id": "mpQv0OFt2630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(chapter6, 'html.parser')\n",
        "\n",
        "for p in soup.find_all(\"p\")[:5]:\n",
        "    print(p.text.strip())"
      ],
      "metadata": {
        "id": "eqCTKFVZ3neO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Fore, Style, init\n",
        "\n",
        "sample = chapter6.strip()[:800]\n",
        "\n",
        "# Mise en forme\n",
        "def stylize(text):\n",
        "    text = re.sub(r'\\bWatson\\b', Fore.GREEN + Style.BRIGHT + \"Watson\" + Style.RESET_ALL, text)\n",
        "    text = re.sub(r'\\bSir\\s+Charles\\b', Fore.CYAN + Style.BRIGHT + \"Sir Charles\" + Style.RESET_ALL, text)\n",
        "    return text\n",
        "\n",
        "# Espacer les lettres comme demandé\n",
        "def expand_text(text):\n",
        "    return ' '.join(list(text))\n",
        "\n",
        "# Styliser, espacer et afficher\n",
        "styled = stylize(sample)\n",
        "#expanded = expand_text(styled)\n",
        "\n",
        "print(Fore.MAGENTA + Style.BRIGHT + \"\\n CHAPITRE 6 : Extrait stylisé\\n\")\n",
        "print(styled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJRZy6hd-V-b",
        "outputId": "8ca6952b-dda8-4086-bc5c-295734d8d405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\u001b[1m\n",
            " CHAPITRE 6 : Extrait stylisé\n",
            "\n",
            "Chapter 6.\r\n",
            "Baskerville Hall\r\n",
            "\r\n",
            "\r\n",
            "      Sir Henry Baskerville and Dr. Mortimer were ready upon the\r\n",
            "      appointed day, and we started as arranged for Devonshire. Mr.\r\n",
            "      Sherlock Holmes drove with me to the station and gave me his last\r\n",
            "      parting injunctions and advice.\r\n",
            "\r\n",
            "      “I will not bias your mind by suggesting theories or suspicions,\r\n",
            "      \u001b[32m\u001b[1mWatson\u001b[0m,” said he; “I wish you simply to report facts in the\r\n",
            "      fullest possible manner to me, and you can leave me to do the\r\n",
            "      theorizing.”\r\n",
            "\r\n",
            "      “What sort of facts?” I asked.\r\n",
            "\r\n",
            "      “Anything which may seem to have a bearing however indirect upon\r\n",
            "      the case, and especially the relations between young Baskerville\r\n",
            "      and his neighbours or any fresh particulars concerning the death\r\n",
            "      of \u001b[36m\u001b[1mSir Charles\u001b[0m. I have ma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse grammaticale et exploration semantique"
      ],
      "metadata": {
        "id": "--xJ0H79BXjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist, word_tokenize\n",
        "raw = requests.get(url).text\n",
        "\n",
        "#  Extraire le texte du Chapitre 9\n",
        "start = raw.find(\"Chapter 9.\")\n",
        "end = raw.find(\"Chapter 10.\")\n",
        "chapter9_raw = raw[start:end]\n",
        "\n",
        "#  Nettoyage du texte\n",
        "chapter9_clean = re.sub(r'[^a-zA-Z\\s]', '', chapter9_raw)  # Supprime la ponctuation\n",
        "chapter9_clean = re.sub(r'\\s+', ' ', chapter9_clean)       # Supprime les espaces multiples\n",
        "chapter9_clean = chapter9_clean.strip()\n",
        "\n",
        "# Tokenisation\n",
        "tokens = word_tokenize(chapter9_clean.lower())  # tout en minuscules\n",
        "\n",
        "#  N-grams\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "print(\" Unigrams les plus fréquents :\", FreqDist(unigrams).most_common(10))\n",
        "print(\" Bigrams les plus fréquents :\", FreqDist(bigrams).most_common(10))\n",
        "print(\" Trigrams les plus fréquents :\", FreqDist(trigrams).most_common(10))\n",
        "\n",
        "#  Binary features (exemples simples)\n",
        "features = {\n",
        "    \"contains_watson\": int(\"watson\" in tokens),\n",
        "    \"contains_holmes\": int(\"holmes\" in tokens),\n",
        "    \"contains_baskerville\": int(\"baskerville\" in tokens),\n",
        "    \"long_word_present\": int(any(len(w) > 10 for w in tokens)),\n",
        "    \"question_present\": int(\"?\" in chapter9_raw),\n",
        "}\n",
        "\n",
        "print(\"\\n Binary Features pour Chapitre 9 :\")\n",
        "for k, v in features.items():\n",
        "    print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4rxshpHBZUD",
        "outputId": "f01be191-6691-4819-898e-883a179cc91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unigrams les plus fréquents : [(('the',), 365), (('and',), 194), (('i',), 183), (('to',), 158), (('of',), 149), (('that',), 148), (('it',), 140), (('a',), 132), (('was',), 128), (('he',), 119)]\n",
            " Bigrams les plus fréquents : [(('of', 'the'), 44), (('in', 'the'), 34), (('it', 'was'), 34), (('that', 'i'), 21), (('sir', 'henry'), 21), (('the', 'moor'), 20), (('i', 'had'), 20), (('it', 'is'), 18), (('that', 'he'), 17), (('he', 'was'), 17)]\n",
            " Trigrams les plus fréquents : [(('on', 'the', 'moor'), 7), (('that', 'it', 'was'), 6), (('upon', 'the', 'moor'), 5), (('it', 'was', 'not'), 5), (('it', 'was', 'a'), 5), (('i', 'could', 'not'), 4), (('the', 'direction', 'of'), 4), (('out', 'of', 'my'), 4), (('out', 'of', 'the'), 4), (('in', 'the', 'darkness'), 4)]\n",
            "\n",
            " Binary Features pour Chapitre 9 :\n",
            "contains_watson : 1\n",
            "contains_holmes : 1\n",
            "contains_baskerville : 1\n",
            "long_word_present : 1\n",
            "question_present : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce script extrait le texte du Chapitre 9 , le nettoie en supprimant la ponctuation et les espaces inutiles, puis effectue plusieurs analyses linguistiques. Il commence par tokeniser le texte en unigrams, bigrams et trigrams, puis affiche les 10 plus fréquents de chaque type. Ensuite, il analyse la présence de certains mots-clés (comme \"watson\", \"holmes\", \"baskerville\") et la présence d'une question dans le texte, ainsi que la présence de mots longs (plus de 10 caractères). Les résultats montrent que les mots les plus fréquents sont des mots de fonction (comme \"the\", \"and\", \"i\"), tandis que les bigrams et trigrams révèlent des expressions récurrentes dans le chapitre, comme \"on the moor\" et \"it was\". Enfin, les binary features montrent que le texte contient bien les mots-clés \"Watson\", \"Holmes\" et \"Baskerville\", ainsi qu'une question et des mots longs."
      ],
      "metadata": {
        "id": "vfAOt8LZ9Cde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZIrubD4Cxul",
        "outputId": "2cca547f-ac1c-4bbe-f757-e688cdc91c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "#  Diviser en phrases\n",
        "sentences = sent_tokenize(chapter9_raw)\n",
        "\n",
        "#  Nettoyage de base + suppression stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cleaned_sentences = []\n",
        "for sent in sentences:\n",
        "    words = word_tokenize(sent.lower())\n",
        "    words = [w for w in words if w.isalpha() and w not in stop_words]\n",
        "    cleaned_sentences.append(words)\n",
        "\n",
        "#  Détection de caractéristiques communes\n",
        "common_features = defaultdict(list)\n",
        "\n",
        "for i in range(len(cleaned_sentences)):\n",
        "    for j in range(i+1, len(cleaned_sentences)):\n",
        "        common = set(cleaned_sentences[i]) & set(cleaned_sentences[j])\n",
        "        if len(common) >= 3:  # ← seuil de similarité\n",
        "            common_features[f\"{i} ⇄ {j}\"] = list(common)\n",
        "\n",
        "#  Résultat\n",
        "print(\"\\n Corrélations entre phrases :\\n\")\n",
        "for pair, common in list(common_features.items())[:10]:  # limiter l'affichage\n",
        "    print(f\" Phrases {pair} ont en commun : {common}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz_tpzHKB6Ls",
        "outputId": "226e3ef9-f7b2-45c5-b5bc-7d99c023d64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Corrélations entre phrases :\n",
            "\n",
            " Phrases 2 ⇄ 38 ont en commun : ['upon', 'holmes', 'leave']\n",
            " Phrases 2 ⇄ 121 ont en commun : ['upon', 'time', 'without']\n",
            " Phrases 2 ⇄ 306 ont en commun : ['acknowledge', 'holmes', 'must', 'dear']\n",
            " Phrases 3 ⇄ 10 ont en commun : ['upon', 'window', 'barrymore']\n",
            " Phrases 10 ⇄ 25 ont en commun : ['must', 'would', 'barrymore']\n",
            " Phrases 21 ⇄ 186 ont en commun : ['heard', 'night', 'window']\n",
            " Phrases 21 ⇄ 198 ont en commun : ['night', 'window', 'every']\n",
            " Phrases 24 ⇄ 227 ont en commun : ['holmes', 'would', 'said']\n",
            " Phrases 25 ⇄ 82 ont en commun : ['man', 'see', 'would']\n",
            " Phrases 25 ⇄ 120 ont en commun : ['case', 'take', 'would']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce script compare les phrases d'un texte en identifiant les mots communs. Après avoir nettoyé les phrases de leurs mots non pertinents (comme les \"stop words\"), il trouve les paires de phrases partageant au moins trois mots. Par exemple, la phrase 2 et la phrase 38 partagent les mots \"upon\", \"holmes\", et \"leave\", ce qui montre qu'elles sont similaires ou liées. Cela aide à analyser les connexions thématiques dans le texte."
      ],
      "metadata": {
        "id": "Li4FbEGP9ehQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUKlTDVqDjbB",
        "outputId": "31306799-5cc3-4a5e-bfaf-576698df144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "# Nettoyer les phrases et faire du POS Tagging\n",
        "tagged_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())  # Tokenisation en mots\n",
        "    tagged_sentences.extend(pos_tag(words))  # POS tagging\n",
        "\n",
        "# Créer des N-grams de POS tags (unigrams, bigrams, trigrams)\n",
        "unigrams = list(ngrams( [tag for word, tag in tagged_sentences], 1))\n",
        "bigrams = list(ngrams([tag for word, tag in tagged_sentences], 2))\n",
        "trigrams = list(ngrams([tag for word, tag in tagged_sentences], 3))\n",
        "\n",
        "#  Afficher les N-grams les plus fréquents\n",
        "print(\"Unigrams (POS tags) les plus fréquents :\")\n",
        "print(FreqDist(unigrams).most_common(10))\n",
        "\n",
        "print(\"Bigrams (POS tags) les plus fréquents :\")\n",
        "print(FreqDist(bigrams).most_common(10))\n",
        "\n",
        "print(\"Trigrams (POS tags) les plus fréquents :\")\n",
        "print(FreqDist(trigrams).most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwG3BepmDt9O",
        "outputId": "9ae23abc-53a6-47a2-a5a7-5ba82ac393d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams (POS tags) les plus fréquents :\n",
            "[(('NN',), 1235), (('IN',), 916), (('DT',), 669), (('PRP',), 596), (('VBD',), 478), (('JJ',), 430), (('RB',), 419), ((',',), 377), (('.',), 349), (('VB',), 328)]\n",
            "Bigrams (POS tags) les plus fréquents :\n",
            "[(('DT', 'NN'), 417), (('IN', 'DT'), 321), (('NN', 'IN'), 271), (('JJ', 'NN'), 208), (('PRP', 'VBD'), 180), (('IN', 'PRP'), 175), (('NN', ','), 164), (('NN', 'VBD'), 157), (('NN', '.'), 155), (('PRP$', 'NN'), 134)]\n",
            "Trigrams (POS tags) les plus fréquents :\n",
            "[(('IN', 'DT', 'NN'), 202), (('DT', 'NN', 'IN'), 137), (('NN', 'IN', 'DT'), 104), (('DT', 'JJ', 'NN'), 92), (('IN', 'DT', 'JJ'), 64), (('IN', 'PRP$', 'NN'), 61), (('JJ', 'NN', 'IN'), 58), (('PRP', 'MD', 'VB'), 55), (('NN', ',', 'CC'), 54), (('DT', 'NN', '.'), 50)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il fait une analyse grammaticale du texte en attribuant des tags POS (part-of-speech) à chaque mot, puis génère des N-grams (unigrams, bigrams, trigrams) à partir de ces tags. Les résultats montrent les combinaisons de tags POS les plus fréquentes, telles que les noms (NN), les prépositions (IN) et les déterminants (DT), ce qui permet d'observer les structures grammaticales récurrentes dans le texte."
      ],
      "metadata": {
        "id": "3-Bjs-zk-SSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-nw_kNTDgD7",
        "outputId": "5cd02212-80fe-4d8b-b406-c9bacd590187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: No module named spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EitUW-z6-te_",
        "outputId": "ba38ffd4-f74e-488e-b976-352417bd7731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from operator import itemgetter\n",
        "\n",
        "# Charger le modèle avec word vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Exemple de texte du chapitre 6\n",
        "chapter6_text = \"\"\"\n",
        "Sir Henry Baskerville and Dr. Mortimer were ready upon the appointed day, and we started as arranged for Devonshire.\n",
        "Mr. Sherlock Holmes drove with me to the station and gave me his last parting injunctions and advice.\n",
        "\"\"\"\n",
        "\n",
        "# Liste des mots-clés du mystère\n",
        "keywords = [\"Holmes\", \"Watson\", \"Baskerville\", \"moor\", \"hound\", \"murder\", \"death\", \"ghost\", \"night\"]\n",
        "\n",
        "# Tokeniser le texte\n",
        "doc = nlp(chapter6_text)\n",
        "\n",
        "# Trouver les mots importants du chapitre\n",
        "words_in_text = list(set([token.text for token in doc if token.is_alpha and not token.is_stop]))\n",
        "\n",
        "# Comparer chaque mot du texte avec nos mots-clés\n",
        "similarities = []\n",
        "\n",
        "for word in words_in_text:\n",
        "    if word in nlp.vocab:  # Vérifie si mot dans le vocabulaire\n",
        "        word_vector = nlp(word)\n",
        "        for key in keywords:\n",
        "            key_vector = nlp(key)\n",
        "            sim = word_vector.similarity(key_vector)\n",
        "            if sim > 0.6:  # Seulement les mots proches\n",
        "                similarities.append((word, key, sim))\n",
        "\n",
        "# Trier et afficher\n",
        "similarities.sort(key=itemgetter(2), reverse=True)\n",
        "print(\"Mots du chapitre proches de nos mots-clés :\\n\")\n",
        "for word, key, sim in similarities:\n",
        "    print(f\"{word}' ≈ '{key}' → Similarité : {sim:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blLuSYB6DUsU",
        "outputId": "2b82794f-9e1e-41b1-d165-5faa979b30f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.1 is installed, but it is not compatible with the installed jaxlib version 0.5.3, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mots du chapitre proches de nos mots-clés :\n",
            "\n",
            "Baskerville' ≈ 'Baskerville' → Similarité : 1.00\n",
            "Henry' ≈ 'Holmes' → Similarité : 1.00\n",
            "Henry' ≈ 'Watson' → Similarité : 1.00\n",
            "drove' ≈ 'death' → Similarité : 1.00\n",
            "Holmes' ≈ 'Holmes' → Similarité : 1.00\n",
            "Holmes' ≈ 'Watson' → Similarité : 1.00\n",
            "day' ≈ 'night' → Similarité : 0.74\n",
            "Henry' ≈ 'moor' → Similarité : 0.64\n",
            "Holmes' ≈ 'moor' → Similarité : 0.64\n",
            "Mortimer' ≈ 'Holmes' → Similarité : 0.60\n",
            "Mortimer' ≈ 'Watson' → Similarité : 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il compare les mots du texte avec une liste de mots-clés (comme \"Holmes\" et \"Baskerville\") en utilisant la similarité de leurs vecteurs de mots. Si la similarité est supérieure à 0,6, il affiche les mots les plus proches, ce qui aide à identifier les termes importants du mystère."
      ],
      "metadata": {
        "id": "7ZHJI-Je_cwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def analogy(wordA, wordB, wordC, top_n=10):\n",
        "    # Vecteurs\n",
        "    vecA = nlp(wordA).vector\n",
        "    vecB = nlp(wordB).vector\n",
        "    vecC = nlp(wordC).vector\n",
        "\n",
        "    # Formule : B - A + C\n",
        "    target_vector = vecB - vecA + vecC\n",
        "\n",
        "    # Chercher dans vocabulaire\n",
        "    similarities = []\n",
        "    for word in nlp.vocab:\n",
        "        if word.has_vector and word.is_lower and word.is_alpha:\n",
        "            sim = dot(target_vector, word.vector) / (norm(target_vector) * norm(word.vector))\n",
        "            similarities.append((word.text, sim))\n",
        "\n",
        "    # Trier\n",
        "    similarities = sorted(similarities, key=lambda item: -item[1])\n",
        "\n",
        "    # Retourner les meilleurs\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Exemple : Holmes est à deduction ce que Watson est à ?\n",
        "results = analogy(\"Holmes\", \"deduction\", \"Watson\", top_n=5)\n",
        "\n",
        "print(\" Holmes est à 'deduction' ce que Watson est à :\")\n",
        "for word, score in results:\n",
        "    print(f\" {word} (score: {score:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf6xI_LdD-rR",
        "outputId": "db953e87-195e-43b6-d4a2-9868d1cb1443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Holmes est à 'deduction' ce que Watson est à :\n",
            " deduction (score: 1.00)\n",
            " you (score: 0.42)\n",
            " ought (score: 0.29)\n",
            " or (score: 0.23)\n",
            " was (score: 0.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous donne une analyse d'analogie de mots basée sur les vecteurs de mots. L'exemple donné recherche la relation entre \"Holmes\" et \"deduction\", puis cherche un mot en relation similaire avec \"Watson\". L'analogie suit la logique de la formule B - A + C, où B et A sont des mots de référence, et C est le mot que l'on cherche à compléter en fonction de cette relation.\n",
        "Dans l'exemple, on essaie de compléter l'analogie \"Holmes est à deduction ce que Watson est à ?\", et le modèle retourne des mots similaires à \"deduction\" dans le contexte de \"Watson\", avec \"deduction\" lui-même étant le meilleur score (1.00). Les mots suivants sont des candidats moins proches, mais encore en lien avec l'analogie.\n",
        "\n"
      ],
      "metadata": {
        "id": "A_bV79tUACQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traduction avec tranformer"
      ],
      "metadata": {
        "id": "koXPIujo7eco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeBQP-PQ765m",
        "outputId": "e3abaf35-ec9a-4dba-8de9-d3aeee5b8ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "# Charger le modèle pré-entraîné pour EN→FR\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptQAbl5S8l-x",
        "outputId": "f8fa4b32-9669-4a90-ff8a-33843e98626b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Bmhztk9TGk",
        "outputId": "7f590fdd-28fd-4e2e-cfc8-201ca7cdbc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = \"\"\"\n",
        "      Sherlock Holmes had, in a very remarkable degree, the power of\n",
        "      detaching his mind at will. For two hours the strange business in\n",
        "      which we had been involved appeared to be forgotten, and he was\n",
        "      entirely absorbed in the pictures of the modern Belgian masters.\n",
        "      He would talk of nothing but art, of which he had the crudest\n",
        "      ideas, from our leaving the gallery until we found ourselves at\n",
        "      the Northumberland Hotel.\n",
        "\"\"\"\n",
        "\n",
        "# Diviser en phrases (tu peux raffiner ça si nécessaire)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = sent_tokenize(english_text)\n",
        "\n",
        "# Traduire phrase par phrase\n",
        "translations = []\n",
        "for sentence in sentences:\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
        "    translated = model.generate(**inputs)\n",
        "    fr_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    translations.append(fr_text)\n",
        "\n",
        "# Afficher la traduction finale\n",
        "\n",
        "for fr in translations:\n",
        "    print(fr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej3e6gbd9ADg",
        "outputId": "a11c6fdc-a9ce-4443-fe4f-dc431751ba3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sherlock Holmes avait, à un degré très remarquable, le pouvoir de détacher son esprit à volonté.\n",
            "Pendant deux heures, l'étrange affaire dans laquelle nous étions impliqués semblait être oubliée, et il était entièrement absorbé dans les images des maîtres belges modernes.\n",
            "Il ne parlait que de l'art, dont il avait les idées les plus grossières, depuis notre départ de la galerie jusqu'à ce que nous nous trouvions à l'hôtel Northumberland.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce script utilise le modèle MarianMT (opus-mt-en-fr) de Helsinki-NLP, basé sur l'architecture Transformer, pour traduire automatiquement un texte anglais en français, phrase par phrase. Il encode chaque phrase, la traduit, puis décode le résultat."
      ],
      "metadata": {
        "id": "9t4XJ0ebDlCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enquete et recherche des mots-clés"
      ],
      "metadata": {
        "id": "Wpwg0fKTQfNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "LirMuGrUGCRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/2852/pg2852.txt\"\n",
        "raw_text = requests.get(url).text\n",
        "\n",
        "\n",
        "chapters = re.split(r\"Chapter\\s+\\d+\\.\", raw_text)[1:]\n",
        "chapters = [\"Chapter \" + str(i+1) + \". \" + chap for i, chap in enumerate(chapters)]\n",
        "\n",
        "# Utiliser les 3 premiers chapitres\n",
        "documents = chapters[:3]\n",
        "\n",
        "# Stopwords anglais\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words, lowercase=True, max_df=0.9, min_df=2)\n",
        "X = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "def top_keywords(chap_index, top_n=10):\n",
        "    row = X[chap_index].toarray().flatten()\n",
        "    top_indices = row.argsort()[-top_n:][::-1]\n",
        "    return [(feature_names[i], row[i]) for i in top_indices]\n",
        "\n",
        "# Afficher les mots-clés de tous les chapitres\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"\\n Mots-clés du Chapitre {i+1} :\\n\")\n",
        "    for word, score in top_keywords(i):\n",
        "        print(f\" {word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng3KNJXXRjKU",
        "outputId": "c7ff0125-3a27-4567-ea27-f8a2984517df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Mots-clés du Chapitre 1 :\n",
            "\n",
            " think: 0.3392\n",
            " visitor: 0.2907\n",
            " dog: 0.2423\n",
            " cross: 0.1938\n",
            " give: 0.1938\n",
            " james: 0.1938\n",
            " hunt: 0.1938\n",
            " behind: 0.1454\n",
            " occasion: 0.1454\n",
            " fellow: 0.1454\n",
            "\n",
            " Mots-clés du Chapitre 2 :\n",
            "\n",
            " baskerville: 0.4565\n",
            " charles: 0.4017\n",
            " hall: 0.2739\n",
            " moor: 0.2374\n",
            " hugo: 0.2374\n",
            " time: 0.1826\n",
            " death: 0.1643\n",
            " hound: 0.1278\n",
            " body: 0.1096\n",
            " seen: 0.1096\n",
            "\n",
            " Mots-clés du Chapitre 3 :\n",
            "\n",
            " moor: 0.3615\n",
            " baskerville: 0.2840\n",
            " charles: 0.2324\n",
            " gate: 0.2066\n",
            " think: 0.1807\n",
            " henry: 0.1549\n",
            " marks: 0.1549\n",
            " day: 0.1549\n",
            " alley: 0.1549\n",
            " evening: 0.1549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "#  Nettoyer + extraire les phrases\n",
        "sentences = sent_tokenize(raw_text)\n",
        "sentences = [s.strip() for s in sentences if len(s.strip()) > 30]  # éviter les phrases très courtes\n",
        "\n",
        "# TF-IDF vectorisation\n",
        "vectorizer = TfidfVectorizer().fit(sentences)\n",
        "sentence_vectors = vectorizer.transform(sentences)\n",
        "\n",
        "def ask_question(question, top_n=2):\n",
        "    print(f\"\\n Question : {question}\")\n",
        "    question_vec = vectorizer.transform([question])\n",
        "    # Use cosine_similarity from sklearn to handle sparse matrices\n",
        "    similarities = cosine_similarity(question_vec, sentence_vectors).flatten()\n",
        "    best_idx = similarities.argsort()[-top_n:][::-1]\n",
        "\n",
        "    print(\"\\n Réponse(s) la plus pertinente trouvée :\\n\")\n",
        "    for i in best_idx:\n",
        "        print( sentences[i], \"\\n\")\n",
        "\n",
        "# Pose tes questions !\n",
        "ask_question(\"Who went to Devonshire?\")\n",
        "ask_question(\"What did Holmes tell Watson?\")\n",
        "ask_question(\"What is Baskerville Hall?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JzfYcd_YKtG",
        "outputId": "2b418f55-dd3d-4033-a0a1-fbc561f157da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Question : Who went to Devonshire?\n",
            "\n",
            " Réponse(s) la plus pertinente trouvée :\n",
            "\n",
            "“The Stapletons then went down to Devonshire, whither they were\r\n",
            "      soon followed by Sir Henry and you. \n",
            "\n",
            "We hope very soon to return to Devonshire. \n",
            "\n",
            "\n",
            " Question : What did Holmes tell Watson?\n",
            "\n",
            " Réponse(s) la plus pertinente trouvée :\n",
            "\n",
            "Why should you mind what they call\r\n",
            "      it?”\r\n",
            "\r\n",
            "      “Tell me, Watson. \n",
            "\n",
            "“But, tell me, Watson, what do you make of\r\n",
            "      our visitor’s stick? \n",
            "\n",
            "\n",
            " Question : What is Baskerville Hall?\n",
            "\n",
            " Réponse(s) la plus pertinente trouvée :\n",
            "\n",
            "Address to Mr. Barrymore, Baskerville Hall. \n",
            "\n",
            "That is Baskerville\r\n",
            "      Hall in the middle.”\r\n",
            "\r\n",
            "      “With a wood round it?”\r\n",
            "\r\n",
            "      “Exactly. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine TF-IDF et cosine similarity pour deux tâches principales :\n",
        "Extraction de mots-clés par chapitre (via TfidfVectorizer)\n",
        "et recherche de réponses dans le texte à des questions simples, en comparant leur similarité vectorielle avec toutes les phrases du roman.\n",
        "C’est une façon efficace de faire de la lecture automatique ou du question answering basique, sans modèle lourd."
      ],
      "metadata": {
        "id": "v7p47pteEyOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QA with hugging face"
      ],
      "metadata": {
        "id": "x_C1nVekMA91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjibz7A4L_Ri",
        "outputId": "851246b9-577d-4abc-81f7-6a9f3d119e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4\n",
        "!pip install torch==2.2.2 torchtext==0.17.2\n",
        "!pip install spacy==3.7.4\n",
        "!pip install transformers==4.39.3\n",
        "!pip install numpy==1.26.4\n",
        "!pip install torch==2.2.2 torchtext==0.17.2\n",
        "!pip install spacy==3.7.4\n",
        "!pip install transformers==4.39.3\n",
        "!pip install sentence-transformers\n",
        "!pip install tensorflow==2.18.0\n",
        "!pip install transformers==4.39.3\n",
        "!pip install sentence-transformers\n",
        "!pip install tensorflow==2.18.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84wk4zcKMpHq",
        "outputId": "8adb5772-aa7d-4a94-db91-6a0601263740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchtext==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.11/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.1.8)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.4) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.7.4) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.2.1)\n",
            "Collecting transformers==4.39.3\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2025.1.31)\n",
            "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.3\n",
            "    Uninstalling transformers-4.50.3:\n",
            "      Successfully uninstalled transformers-4.50.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.0.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.39.3\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchtext==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.11/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.1.8)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.4) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.7.4) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.2.1)\n",
            "Requirement already satisfied: transformers==4.39.3 in /usr/local/lib/python3.11/dist-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2025.1.31)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.51.2\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
            "Collecting transformers==4.39.3\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2025.1.31)\n",
            "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.2\n",
            "    Uninstalling transformers-4.51.2:\n",
            "      Successfully uninstalled transformers-4.51.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.0.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.39.3\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Using cached transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Using cached transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.51.2\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"Who went to Devonshire?\"\n",
        "question2= \"What did Holmes tell Watson?\"\n",
        "question3 = \"What is Baskerville Hall?\"\n",
        "\n",
        "context = cleaned_text[:5000]\n",
        "\n",
        "\n",
        "result1 = qa_pipeline(question=question1, context=context)\n",
        "result2 = qa_pipeline(question=question2, context=context)\n",
        "result3 = qa_pipeline(question=question3, context=context)\n",
        "print(result1)\n",
        "print(result2)\n",
        "print(result3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtys57ONMJYp",
        "outputId": "ee510e41-6b43-4642-ce56-31cf1abbdae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.023909490555524826, 'start': 3495, 'end': 3501, 'answer': 'Holmes'}\n",
            "{'score': 0.007883720099925995, 'start': 2151, 'end': 2170, 'answer': 'our visitor’s stick'}\n",
            "{'score': 0.00847458466887474, 'start': 2484, 'end': 2496, 'answer': 'Dr. Mortimer'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code utilise un modèle pré-entraîné (deepset/roberta-base-squad2) pour répondre à des questions en langage naturel. Utile pour retrouver une info précise sans lire tout le roman."
      ],
      "metadata": {
        "id": "O51urj3LG3jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table QA"
      ],
      "metadata": {
        "id": "9GBNCPuiJiP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# 1. Créer le tableau traduit en anglais\n",
        "data = {\n",
        "    \"Character\": [\"Sherlock Holmes\", \"Dr. Watson\", \"Sherlock Holmes\"],\n",
        "    \"Location\": [\"Art Gallery\", \"Art Gallery\", \"Northumberland Hotel\"],\n",
        "    \"Activity\": [\"Observe paintings\", \"Accompany\", \"Arrive\"],\n",
        "    \"Duration\": [\"2 hours\", \"2 hours\", \"Not specified\"]\n",
        "}\n",
        "table = pd.DataFrame(data)\n",
        "\n",
        "# 2. Définir les questions en anglais\n",
        "questions = [\n",
        "    \"What is the location where Sherlock Holmes spent 2 hours?\",\n",
        "    \"What activity did Dr. Watson do at the art gallery?\",\n",
        "    \"What is the duration Sherlock Holmes spent at the art gallery?\",\n",
        "    \"What is the location Sherlock Holmes went to after the art gallery?\"\n",
        "]\n",
        "\n",
        "# 3. Charger le modèle et le tokenizer TAPAS\n",
        "model_name = \"google/tapas-base-finetuned-wtq\"  # Retour à la base pour tester\n",
        "tokenizer = TapasTokenizer.from_pretrained(model_name)\n",
        "model = TapasForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# 4. Fonction pour répondre aux questions avec filtrage\n",
        "def answer_questions(table, questions):\n",
        "    table = table.astype(str)\n",
        "\n",
        "    for question in questions:\n",
        "        try:\n",
        "            # Tokeniser\n",
        "            inputs = tokenizer(\n",
        "                table=table,\n",
        "                queries=question,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Prédiction\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Extraire les coordonnées\n",
        "            coordinates, _ = tokenizer.convert_logits_to_predictions(\n",
        "                inputs, outputs.logits, outputs.logits_aggregation\n",
        "            )\n",
        "            coordinates = coordinates[0] if coordinates else []\n",
        "\n",
        "            # Filtrer les réponses selon la colonne attendue\n",
        "            answers = []\n",
        "            expected_column = None\n",
        "            if \"location\" in question.lower():\n",
        "                expected_column = \"Location\"\n",
        "            elif \"activity\" in question.lower():\n",
        "                expected_column = \"Activity\"\n",
        "            elif \"duration\" in question.lower():\n",
        "                expected_column = \"Duration\"\n",
        "\n",
        "            for coord in coordinates:\n",
        "                try:\n",
        "                    row, col = coord\n",
        "                    column_name = table.columns[col]\n",
        "                    cell_value = table.iloc[row, col]\n",
        "                    # Vérifier si la colonne correspond à l'attente\n",
        "                    if expected_column is None or column_name == expected_column:\n",
        "                        answers.append(cell_value)\n",
        "                    print(f\"[Debug] Cellule sélectionnée pour '{question}' : Ligne {row}, Colonne {column_name} = {cell_value}\")\n",
        "                except ValueError as e:\n",
        "                    print(f\"[Debug] Erreur de coordonnée pour '{question}' : {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Afficher la question et la réponse\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {', '.join(answers) if answers else 'No answer'}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Debug] Erreur pour la question '{question}' : {str(e)}\")\n",
        "            print(f\"Answer: No answer\\n\")\n",
        "\n",
        "# 5. Exécuter\n",
        "print(\"Table:\")\n",
        "print(table)\n",
        "print(\"\\nAnswers to questions:\")\n",
        "answer_questions(table, questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Mwsy9JIq8_",
        "outputId": "d3737653-f955-49f9-c725-d485ecf586fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table:\n",
            "         Character              Location           Activity       Duration\n",
            "0  Sherlock Holmes           Art Gallery  Observe paintings        2 hours\n",
            "1       Dr. Watson           Art Gallery          Accompany        2 hours\n",
            "2  Sherlock Holmes  Northumberland Hotel             Arrive  Not specified\n",
            "\n",
            "Answers to questions:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  text = normalize_for_match(row[col_index].text)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  cell = row[col_index]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Cellule sélectionnée pour 'What is the location where Sherlock Holmes spent 2 hours?' : Ligne 0, Colonne Location = Art Gallery\n",
            "Question: What is the location where Sherlock Holmes spent 2 hours?\n",
            "Answer: Art Gallery\n",
            "\n",
            "[Debug] Cellule sélectionnée pour 'What activity did Dr. Watson do at the art gallery?' : Ligne 1, Colonne Activity = Accompany\n",
            "Question: What activity did Dr. Watson do at the art gallery?\n",
            "Answer: Accompany\n",
            "\n",
            "[Debug] Cellule sélectionnée pour 'What is the duration Sherlock Holmes spent at the art gallery?' : Ligne 0, Colonne Duration = 2 hours\n",
            "Question: What is the duration Sherlock Holmes spent at the art gallery?\n",
            "Answer: 2 hours\n",
            "\n",
            "[Debug] Cellule sélectionnée pour 'What is the location Sherlock Holmes went to after the art gallery?' : Ligne 2, Colonne Location = Northumberland Hotel\n",
            "Question: What is the location Sherlock Holmes went to after the art gallery?\n",
            "Answer: Northumberland Hotel\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code utilise le modèle TAPAS de Google, spécialisé dans l’interrogation directe de tableaux (comme ceux d’Excel). Il peut répondre à des questions en langage naturel en sélectionnant les bonnes cellules d’un tableau de données. Ici, on observe qu’il répond bien aux questions."
      ],
      "metadata": {
        "id": "VZ2_UG_jHqk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA"
      ],
      "metadata": {
        "id": "UA0UxthzGtdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Téléchargez les ressources nécessaires pour NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 📥 Charger le roman depuis une URL ou fichier\n",
        "url = \"https://www.gutenberg.org/cache/epub/2852/pg2852.txt\"  # URL du roman Sherlock Holmes\n",
        "raw = requests.get(url).text\n",
        "\n",
        "# 📖 Extraire le chapitre 6\n",
        "start = raw.find(\"Chapter 6.\")\n",
        "end = raw.find(\"Chapter 7.\")\n",
        "chapter6 = raw[start:end]\n",
        "\n",
        "# Tokenisation et nettoyage du texte\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = nltk.word_tokenize(chapter6)\n",
        "\n",
        "# Nettoyage des tokens (enlever les caractères non alphabétiques et les stopwords)\n",
        "tokens_cleaned = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "cleaned_text = \" \".join(tokens_cleaned)\n",
        "\n",
        "# Créer un vecteur de compte de mots (Bag-of-Words)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform([cleaned_text])\n",
        "\n",
        "# Appliquer le modèle LDA pour détecter 3 thèmes\n",
        "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "#  Afficher les mots-clés de chaque thème\n",
        "n_top_words = 10\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"\\n Thème {topic_idx + 1}:\")\n",
        "    print(\" \".join([terms[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiZBRIUsaaft",
        "outputId": "b5c77430-be5b-4646-9a28-14d2341062a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Thème 1:\n",
            "accent account yew yesterday ancestors ancient answer apartment appeared appears\n",
            "\n",
            " Thème 2:\n",
            "sir baskerville hall said henry long mortimer light old house\n",
            "\n",
            " Thème 3:\n",
            "accent account yew yesterday ancestors ancient answer apartment appeared appears\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse thématique avec LDA, il détecte automatiquement les thèmes dominants.\n",
        "Il applique une vectorisation bag-of-words, supprime les mots inutiles (stopwords), puis identifie 3 groupes de mots les plus représentatifs, correspondant aux sujets récurrents du chapitre."
      ],
      "metadata": {
        "id": "8aIawlnfIoMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "v2evI7O8cA5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "texts = [\n",
        "    \"Sherlock Holmes investigates the strange death of Sir Charles Baskerville\",\n",
        "    \"The mystery of the Baskerville family continues to haunt the countryside\",\n",
        "    \"A family in turmoil after the tragic death of Sir Charles Baskerville\",\n",
        "    \"Dr. Watson is concerned about the strange behavior of Sir Charles\",\n",
        "    \"The Baskerville case takes an unexpected turn with new evidence\",\n",
        "    \"Holmes uses logic to solve a complex murder mystery\",\n",
        "    \"The town is filled with drama as the case unfolds\",\n",
        "    \"Suspicion falls on various individuals, and tension rises in the village\",\n",
        "    \"A quiet and peaceful village becomes a stage for a dramatic crime\",\n",
        "    \"The murder mystery leads to a shocking revelation\",\n",
        "    \"The detective uncovers new clues in the case\",\n",
        "    \"Suspense builds as the story of a haunted mansion unfolds\"\n",
        "]\n",
        "\n",
        "# Cibles (1: Mystère, 0: Drame)\n",
        "labels = [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
        "\n",
        "# Prétraitement des données : nettoyage et lemmatisation\n",
        "def preprocess(text):\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "    # Supprimer les caractères spéciaux, chiffres, etc.\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenisation et suppression des stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatisation\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Appliquer le prétraitement\n",
        "texts = [preprocess(text) for text in texts]\n",
        "\n",
        "# Vectorisation du texte avec TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Diviser en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Définir les classificateurs\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "nb_classifier = MultinomialNB()\n",
        "logreg_classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Pipeline pour la vectorisation et les modèles\n",
        "pipelines = {\n",
        "    'Random Forest': Pipeline([\n",
        "        ('vectorizer', vectorizer),\n",
        "        ('classifier', rf_classifier)\n",
        "    ]),\n",
        "    'Decision Tree': Pipeline([\n",
        "        ('vectorizer', vectorizer),\n",
        "        ('classifier', dt_classifier)\n",
        "    ]),\n",
        "    'Naive Bayes': Pipeline([\n",
        "        ('vectorizer', vectorizer),\n",
        "        ('classifier', nb_classifier)\n",
        "    ]),\n",
        "    'Logistic Regression': Pipeline([\n",
        "        ('vectorizer', vectorizer),\n",
        "        ('classifier', logreg_classifier)\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Ajouter un classifier Voting avec tous les modèles\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', rf_classifier),\n",
        "    ('dt', dt_classifier),\n",
        "    ('nb', nb_classifier),\n",
        "    ('logreg', logreg_classifier)\n",
        "], voting='hard')\n",
        "\n",
        "pipelines['Voting Classifier'] = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', voting_clf)\n",
        "])\n",
        "\n",
        "# Dictionnaire pour stocker les résultats\n",
        "results = {}\n",
        "\n",
        "# Entraîner et évaluer chaque modèle\n",
        "for name, pipeline in pipelines.items():\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calcul des performances\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    }\n",
        "\n",
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n",
        "\n",
        "# Tester avec un texte inconnu pour prédire\n",
        "new_texts = [\n",
        "    \"A detective finds clues in a dark mansion\",\n",
        "    \"A couple struggles with their relationship in a tragic setting\",\n",
        "    \"Holmes uncovers a dangerous secret in the case of Sir Charles\",\n",
        "    \"The tension in the village rises after the mysterious death\"\n",
        "]\n",
        "\n",
        "# Preprocess the new texts\n",
        "new_texts_preprocessed = [preprocess(text) for text in new_texts]\n",
        "\n",
        "# Vectorize the preprocessed new texts using the fitted vectorizer\n",
        "new_texts_vectorized = vectorizer.transform(new_texts_preprocessed) # added this line\n",
        "\n",
        "# Make predictions\n",
        "predictions = voting_clf.predict(new_texts_vectorized) # changed input to new_texts_vectorized\n",
        "predictions_labels = label_encoder.inverse_transform(predictions)\n",
        "\n",
        "# Afficher les prédictions\n",
        "for text, label in zip(new_texts, predictions_labels):\n",
        "    print(f\"Text: '{text}' => Predicted Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1X8zpx7wvV3",
        "outputId": "e175e319-aff2-4bd2-8dcf-86935bbbeb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Accuracy  Precision    Recall  F1-Score\n",
            "Random Forest        0.000000        0.0  0.000000       0.0\n",
            "Decision Tree        0.666667        1.0  0.666667       0.8\n",
            "Naive Bayes          0.333333        1.0  0.333333       0.5\n",
            "Logistic Regression  0.000000        0.0  0.000000       0.0\n",
            "Voting Classifier    0.000000        0.0  0.000000       0.0\n",
            "Text: 'A detective finds clues in a dark mansion' => Predicted Label: 0\n",
            "Text: 'A couple struggles with their relationship in a tragic setting' => Predicted Label: 0\n",
            "Text: 'Holmes uncovers a dangerous secret in the case of Sir Charles' => Predicted Label: 0\n",
            "Text: 'The tension in the village rises after the mysterious death' => Predicted Label: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit d'un petit exemple pour illustrer la manière dont je peux réaliser une tâche de classification de texte. L'objectif est de classer des phrases en deux catégories : Mystère (1) et Drame (0), en utilisant divers modèles de classification. Après un prétraitement des données comprenant la conversion en minuscules, la suppression des stopwords et la lemmatisation, les textes ont été transformés en vecteurs numériques via la méthode TF-IDF.\n",
        "Plusieurs modèles ont été entraînés pour cette tâche, parmi lesquels : Random Forest, Decision Tree, Naive Bayes, Logistic Regression, ainsi qu’un Voting Classifier combinant les prédictions de tous ces modèles. Les performances des modèles ont été évaluées à l’aide de diverses métriques, telles que l'Accuracy, la Precision, le Recall et le F1-Score.\n",
        "Les résultats ont révélé que le modèle Decision Tree était le plus performant, avec une précision de 66 % et un F1-score de 0,8. En revanche, les autres modèles ont obtenu une précision de 0 %, prédisant systématiquement la classe \"Drame\", ce qui suggère un biais notable dans leurs prédictions. Cela est probablement dû à un déséquilibre dans les données et à la taille réduite de l’échantillon utilisé pour l’entraînement."
      ],
      "metadata": {
        "id": "p1QBQUfYLWVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import random\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# 1. Données\n",
        "mystery_texts = [\n",
        "    \"A shadowy figure disappears into the fog.\",\n",
        "    \"A locked room with no way in or out.\",\n",
        "    \"Footprints lead to a dead end in the snow.\",\n",
        "    \"Holmes studies a mysterious letter.\",\n",
        "    \"Someone is watching from the dark hallway.\",\n",
        "    \"A secret passage behind the bookshelf.\"\n",
        "]\n",
        "\n",
        "drama_texts = [\n",
        "    \"A son confronts his father after years apart.\",\n",
        "    \"She weeps as the letter is read aloud.\",\n",
        "    \"The lovers must part under tragic circumstances.\",\n",
        "    \"He regrets the words he never said.\",\n",
        "    \"Tears fall during the final goodbye.\",\n",
        "    \"A child is caught in a family feud.\"\n",
        "]\n",
        "\n",
        "# Fusionner\n",
        "data = [(txt, 'mystery') for txt in mystery_texts] + [(txt, 'drama') for txt in drama_texts]\n",
        "\n",
        "\n",
        "# 2. Prétraitement\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def extract_features(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words]\n",
        "    return {word: True for word in tokens}\n",
        "\n",
        "# Extraire les features\n",
        "featuresets = [(extract_features(text), label) for (text, label) in data]\n",
        "\n",
        "# Mélanger aléatoirement\n",
        "random.shuffle(featuresets)\n",
        "\n",
        "# Split train/test\n",
        "split_point = int(len(featuresets) * 0.8)\n",
        "train_set = featuresets[:split_point]\n",
        "test_set = featuresets[split_point:]\n",
        "\n",
        "\n",
        "# 3. MaxEnt Classifier\n",
        "maxent_classifier = MaxentClassifier.train(train_set, algorithm='IIS', trace=0, max_iter=1000)\n",
        "\n",
        "# Évaluation MaxEnt\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for features, label in test_set:\n",
        "    y_true.append(label)\n",
        "    y_pred.append(maxent_classifier.classify(features))\n",
        "\n",
        "print(\"\\n MaxEnt Classifier Evaluation\")\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred, pos_label='mystery', zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred, pos_label='mystery', zero_division=0))\n",
        "print(\"F1-Score:\", f1_score(y_true, y_pred, pos_label='mystery', zero_division=0))\n",
        "\n",
        "\n",
        "# 4. MultiBinaryClassifier (OvR avec Naive Bayes)\n",
        "labels = list(set(label for _, label in featuresets))\n",
        "binary_classifiers = {}\n",
        "\n",
        "for label in labels:\n",
        "    binary_train = [(\n",
        "        feats,\n",
        "        (lbl == label)\n",
        "    ) for feats, lbl in train_set]\n",
        "\n",
        "    clf = NaiveBayesClassifier.train(binary_train)\n",
        "    binary_classifiers[label] = clf\n",
        "\n",
        "def predict_multibinary(feats):\n",
        "    probs = {label: clf.prob_classify(feats).prob(True) for label, clf in binary_classifiers.items()}\n",
        "    return max(probs, key=probs.get)\n",
        "\n",
        "# Évaluation MultiBinary\n",
        "y_true_multi = []\n",
        "y_pred_multi = []\n",
        "\n",
        "for features, label in test_set:\n",
        "    pred = predict_multibinary(features)\n",
        "    y_true_multi.append(label)\n",
        "    y_pred_multi.append(pred)\n",
        "\n",
        "print(\"\\n MultiBinary Classifier Evaluation (OvR)\")\n",
        "print(\"Accuracy:\", accuracy_score(y_true_multi, y_pred_multi))\n",
        "print(\"Precision:\", precision_score(y_true_multi, y_pred_multi, pos_label='mystery', zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_true_multi, y_pred_multi, pos_label='mystery', zero_division=0))\n",
        "print(\"F1-Score:\", f1_score(y_true_multi, y_pred_multi, pos_label='mystery', zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpjbAph6zz_K",
        "outputId": "e05c33ac-db3a-4ff4-bc9e-b3f24bfe0a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " MaxEnt Classifier Evaluation\n",
            "Accuracy: 0.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n",
            "\n",
            " MultiBinary Classifier Evaluation (OvR)\n",
            "Accuracy: 0.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce cas, j'ai appliqué les modèles suivants :\n",
        "* MaxEnt Classifier : Un classificateur basé sur l'entropie maximale a été utilisé pour l’entraînement. Ce modèle apprend à partir des données d’entraînement, puis est évalué sur l’ensemble de test.\n",
        "* MultiBinary Classifier (Naive Bayes OvR) : Il s'agit d'un classificateur One-vs-Rest (OvR), où chaque catégorie est traitée indépendamment. Ce modèle génère des classificateurs binaires pour chaque classe et prédit celle ayant la plus grande probabilité.\n",
        "\n",
        "Cependant, les résultats indiquent que ces modèles ne sont pas suffisamment performants lorsqu'ils sont appliqués à de petits jeux de données. Ils nécessitent un entraînement plus approfondi et une plus grande diversité dans les textes pour améliorer leurs performances."
      ],
      "metadata": {
        "id": "oj4rUOByMld6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "-Wge9sDExl74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "phrases = [\n",
        "    \"Sherlock Holmes investigates a strange case\",\n",
        "    \"The Baskerville family is in danger and Watson is concerned\",\n",
        "    \"Dr. Mortimer speaks of strange occurrences\",\n",
        "    \"Sir Charles Baskerville is found dead under mysterious circumstances\",\n",
        "    \"Holmes uses his reasoning to solve the case\"\n",
        "]\n",
        "\n",
        "# Longueur des phrases (en mots)\n",
        "lengths = [len(phrase.split()) for phrase in phrases]\n",
        "\n",
        "# Vectorisation du texte avec TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(phrases)\n",
        "\n",
        "# Diviser en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, lengths, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créer et entraîner les modèles de régression\n",
        "# Régression Linéaire\n",
        "linear_regressor = LinearRegression()\n",
        "linear_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Lasso Regressor\n",
        "lasso_regressor = Lasso(alpha=0.1)\n",
        "lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Prédictions\n",
        "y_pred_linear = linear_regressor.predict(X_test)\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "y_pred_lasso = lasso_regressor.predict(X_test)\n",
        "\n",
        "# Calcul des erreurs quadratiques moyennes (RMSE)\n",
        "rmse_linear = mean_squared_error(y_test, y_pred_linear) ** 0.5\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf) ** 0.5\n",
        "rmse_lasso = mean_squared_error(y_test, y_pred_lasso) ** 0.5\n",
        "\n",
        "# Affichage des résultats\n",
        "print(f\"RMSE de la régression linéaire : {rmse_linear}\")\n",
        "print(f\"RMSE du Random Forest Regressor : {rmse_rf}\")\n",
        "print(f\"RMSE du Lasso Regressor : {rmse_lasso}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3aDw1zzR_H",
        "outputId": "9c71ad22-59d7-4552-a11d-d0da03a0e615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE de la régression linéaire : 2.201949260116556\n",
            "RMSE du Random Forest Regressor : 2.3099999999999996\n",
            "RMSE du Lasso Regressor : 2.0479239489797374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai réalisé une prédiction de la longueur des phrases en nombre de mots en utilisant trois modèles de régression : régression linéaire, Random Forest Regressor et Lasso Regressor. Le texte est d'abord transformé en vecteurs numériques à l'aide de TF-IDF. Ensuite, les données sont divisées en ensembles d'entraînement et de test. Chaque modèle est entraîné pour prédire la longueur des phrases, et leur performance est mesurée à l'aide de l'erreur quadratique moyenne (RMSE).\n",
        "Les résultats montrent que le Lasso Regressor obtient le meilleur RMSE (2.05), suivi de près par la régression linéaire (2.20) et le Random Forest (2.31). Un RMSE plus faible indique une meilleure précision des prédictions."
      ],
      "metadata": {
        "id": "se1dTel2NzRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "lJCayvZ85Ri9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK+Naive bayesian classifier"
      ],
      "metadata": {
        "id": "Yra3ElVx5V5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Préparer les données\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Extraire les caractéristiques\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    return {w: (w in words) for w in word_features}\n",
        "\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Train/test split\n",
        "train_set, test_set = featuresets[:1500], featuresets[1500:]\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "print(\"Accuracy:\", accuracy(classifier, test_set))\n",
        "classifier.show_most_informative_features(10)\n",
        "\n",
        "# Test avec un extrait de roman :\n",
        "test_text = \"Her heart sank as she read the final letter from him\"\n",
        "features = document_features(test_text.split())\n",
        "print(\"Sentiment:\", classifier.classify(features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYc8A7cB5UnZ",
        "outputId": "b170c312-b41e-411d-fa41-1b7a9b05237a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.796\n",
            "Most Informative Features\n",
            "             outstanding = True              pos : neg    =      8.8 : 1.0\n",
            "             wonderfully = True              pos : neg    =      8.2 : 1.0\n",
            "                   mulan = True              pos : neg    =      7.3 : 1.0\n",
            "                  truman = True              pos : neg    =      7.3 : 1.0\n",
            "                  seagal = True              neg : pos    =      7.1 : 1.0\n",
            "                  wasted = True              neg : pos    =      6.7 : 1.0\n",
            "                  poorly = True              neg : pos    =      6.6 : 1.0\n",
            "                   damon = True              pos : neg    =      6.1 : 1.0\n",
            "                   awful = True              neg : pos    =      6.0 : 1.0\n",
            "                   hanks = True              pos : neg    =      5.8 : 1.0\n",
            "Sentiment: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging face"
      ],
      "metadata": {
        "id": "MDyLq6cN5pXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Pipeline pré-entraîné pour sentiment analysis\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Exemple avec ton roman\n",
        "text = \"She felt hopeless and broken after the incident\"\n",
        "result = classifier(text)\n",
        "print(result)  # [{'label': 'NEGATIVE', 'score': 0.999...}]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "2830c0d0207b43829e5edb31e4214386",
            "a58d60f9934c46e2bf6c6ecc1912fb27",
            "040d4b33cbec4599b0233999a09d4023",
            "1d2facb24ee143c6a1f18eb8c14204c7",
            "12da2c9205894cf4b6bc9b1b34828913",
            "f1ed83aa4737412d952928c0f4acbcc9",
            "c5bc37521bc84b3c98e9335ee320666b",
            "4ef7fdc9433246679eef2f3e97c6ffce",
            "efad2f30eb024498be344ad8b251e957",
            "270996a1243a431cb8bc1f475a940ab6",
            "a0744227e6b44eb0a582aac7fe3dbc88",
            "14b405539fa341dd991829f1427ad879",
            "6fb517053ebf4473b77270ae53a12dfb",
            "cbee7f34885243dfb67dbabd674de70f",
            "9bd820f754b84210a4e70301b1de2edd",
            "4b3a7b6520c641fb944e707e60f75f1b",
            "aa5919cf542241f5b2b2ebaa5afece78",
            "503a448c81f342b6b235d70f9c4a2f4c",
            "1a0ba0825df44d61ad5ac2bb19ddc1a6",
            "6abdb76ef8814cc696e26b6215bcaf91",
            "9ca07039c9014068b64f6dc721b2db39",
            "2da309823f25480c927b3d71c0c7e090",
            "cc980a6b17484b4b8970a79b7d62e467",
            "4e636e0be67d42b1969f08549be0c669",
            "0acf199117e64be5b1c9f5e4c0b35408",
            "3d1505768d2647028eb1e27b9e1204bf",
            "5d0ba503b0bc4b168efbb252774ace36",
            "f4680952c5b84d8f8ab233f7caae012a",
            "c0a171df34eb4ea2a243154a5b6d7b90",
            "e8a6885adb4b42f28c57e9ee677fcd4e",
            "df6e26c5da3a484ea109eb6b3078f9e8",
            "55e4d1a7ceda4526b3022c916441d89c",
            "3146b6474a774beb9bc3952ddd968f71",
            "775f2a963f0f4f9687e3eec1495e4103",
            "ac70b3c74a114a539b4bdc6ebcb29a13",
            "e122de7f1ed74404b595b33aff39ec0e",
            "1f4d1237ffa64ada801d0699c8a6f773",
            "a689bf1a1987405199a66cf999d9b082",
            "2181f636fd6b4a4b817b267a9224d995",
            "b85d3996b14d4a63af4f1218bdf306fc",
            "94a1c8720072493fa4ccab5fca92e3ea",
            "05c493ecbb074a7aa8a43089fcfc8089",
            "72f999430c6a42e4b895f7d3a486f82c",
            "54bd7129978746b1ae80f0749f9a9155"
          ]
        },
        "id": "lQ9vmgrm5oZ_",
        "outputId": "36d0df05-e1e2-4a1f-f185-dedd2bb79922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2830c0d0207b43829e5edb31e4214386"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14b405539fa341dd991829f1427ad879"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc980a6b17484b4b8970a79b7d62e467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "775f2a963f0f4f9687e3eec1495e4103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9993752837181091}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "geplPDv557py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation auto avec hugging face\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "annotated = []\n",
        "\n",
        "for sent in sentences[:500]:  # Limite pour tester\n",
        "    result = classifier(sent)[0]\n",
        "    label = result['label'].lower()  # 'POSITIVE', 'NEGATIVE'\n",
        "    annotated.append((sent, label))\n",
        "\n",
        "print(annotated[:100])"
      ],
      "metadata": {
        "id": "NIgLT4dO7dvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### creation un csv\n",
        "import csv\n",
        "\n",
        "with open(\"hound_sentiment.csv\", \"w\", newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"text\", \"label\"])\n",
        "    for sent, label in annotated:\n",
        "        writer.writerow([sent, label])"
      ],
      "metadata": {
        "id": "WyBYXi0V_ALQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchtext torchvision torchaudio timm fastai sentence-transformers accelerate peft -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjdPf_TZGFXH",
        "outputId": "76f06e0a-41c1-4f86-ed79-2fb009144a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchtext 0.17.2\n",
            "Uninstalling torchtext-0.17.2:\n",
            "  Successfully uninstalled torchtext-0.17.2\n",
            "Found existing installation: torchvision 0.17.2\n",
            "Uninstalling torchvision-0.17.2:\n",
            "  Successfully uninstalled torchvision-0.17.2\n",
            "Found existing installation: torchaudio 2.2.2\n",
            "Uninstalling torchaudio-2.2.2:\n",
            "  Successfully uninstalled torchaudio-2.2.2\n",
            "Found existing installation: timm 1.0.15\n",
            "Uninstalling timm-1.0.15:\n",
            "  Successfully uninstalled timm-1.0.15\n",
            "Found existing installation: fastai 2.7.19\n",
            "Uninstalling fastai-2.7.19:\n",
            "  Successfully uninstalled fastai-2.7.19\n",
            "Found existing installation: sentence-transformers 4.0.2\n",
            "Uninstalling sentence-transformers-4.0.2:\n",
            "  Successfully uninstalled sentence-transformers-4.0.2\n",
            "Found existing installation: accelerate 1.6.0\n",
            "Uninstalling accelerate-1.6.0:\n",
            "  Successfully uninstalled accelerate-1.6.0\n",
            "Found existing installation: peft 0.15.1\n",
            "Uninstalling peft-0.15.1:\n",
            "  Successfully uninstalled peft-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2 torchvision torchaudio torchtext==0.17.2\n",
        "!pip install timm fastai sentence-transformers accelerate peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VOuFR8IWGIci",
        "outputId": "69161bea-cb32-477c-bd3c-981866b1ffd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchtext==0.17.2\n",
            "  Using cached torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.20.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Using cached torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Using cached torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.4.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "Using cached torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
            "Using cached torchaudio-2.2.2-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "docling-ibm-models 3.4.1 requires transformers<5.0.0,>=4.42.0; sys_platform != \"darwin\" or platform_machine != \"x86_64\", but you have transformers 4.31.0 which is incompatible.\n",
            "spacy-transformers 1.2.5 requires transformers<4.31.0,>=3.4.0, but you have transformers 4.31.0 which is incompatible.\n",
            "en-core-web-trf 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.8.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.2 torchaudio-2.2.2 torchtext-0.17.2 torchvision-0.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "60563b968b444773a3749e1f648b4b1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Using cached timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
            "Collecting fastai\n",
            "  Using cached fastai-2.7.19-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft\n",
            "  Using cached peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.17.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastai) (24.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.7.29)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fastai) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fastai) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fastai) (2.32.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from fastai) (10.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fastai) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fastai) (1.14.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.11/dist-packages (from fastai) (3.8.5)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Using cached transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.2.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2024.12.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.9.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (2025.1.31)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données avec pandas\n",
        "df = pd.read_csv(\"hound_sentiment.csv\")  # Assure-toi que les colonnes sont 'text' et 'label'\n",
        "\n",
        "# Tokenizer spaCy\n",
        "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "\n",
        "# Construction du vocabulaire\n",
        "def yield_tokens(texts):\n",
        "    for text in texts:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Split train/test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build vocab\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Mapping des labels\n",
        "label_vocab = {label: idx for idx, label in enumerate(sorted(set(train_labels)))}\n",
        "inv_label_vocab = {v: k for k, v in label_vocab.items()}\n",
        "\n",
        "# Dataset personnalisé\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, vocab, label_vocab):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.texts[idx])\n",
        "        indices = [self.vocab[token] for token in tokens]\n",
        "        label = self.label_vocab[self.labels[idx]]\n",
        "        return torch.tensor(indices), torch.tensor(label)\n",
        "\n",
        "# Collate function pour padding dynamique\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list = [], []\n",
        "    for text, label in batch:\n",
        "        text_list.append(text)\n",
        "        label_list.append(label)\n",
        "    text_batch = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    label_batch = torch.tensor(label_list)\n",
        "    return text_batch, label_batch\n",
        "\n",
        "# Instancier les datasets\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer, vocab, label_vocab)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer, vocab, label_vocab)\n",
        "\n",
        "# DataLoaders\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Modèle RNN\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, _) = self.rnn(embedded)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "# Instancier le modèle\n",
        "model = SentimentRNN(len(vocab), 100, 128, len(label_vocab)).to(device)\n",
        "\n",
        "# Optimizer et Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1hkCXrFEw0P",
        "outputId": "fd640f62-afd2-408c-e28a-ae9dd17a05bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-84a639206bf7>\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Entraînement du modèle\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for texts, labels in dataloader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "ttQRXIVxHVxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Évaluation\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in dataloader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n"
      ],
      "metadata": {
        "id": "z_GjawBoJtrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Boucle d'entraînement principale\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}%, Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md5CRQJ8HgkE",
        "outputId": "a19403c8-32ba-484e-a9d3-8a74c0102bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=0.6999, Acc=47.00%, Val Loss=0.6628, Acc=60.00%\n",
            "Epoch 2: Train Loss=0.6881, Acc=54.75%, Val Loss=0.6727, Acc=60.00%\n",
            "Epoch 3: Train Loss=0.6850, Acc=55.00%, Val Loss=0.6568, Acc=60.00%\n",
            "Epoch 4: Train Loss=0.6854, Acc=55.00%, Val Loss=0.6477, Acc=60.00%\n",
            "Epoch 5: Train Loss=0.6822, Acc=55.50%, Val Loss=0.6531, Acc=59.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### predire un sentiment\n",
        "def predict_sentiment(sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence)\n",
        "    indices = torch.tensor([vocab[token] for token in tokens]).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(indices)\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "    return inv_label_vocab[pred]\n",
        "\n",
        "# Test\n",
        "print(predict_sentiment(\"He was terrified by the howling wind and darkness.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgUXEG2sJ9Rp",
        "outputId": "18b1462d-7b40-4bfd-96cd-b3382ee89175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette partie, j'ai effectué une analyse de sentiment sur des phrases en utilisant trois méthodes différentes.\n",
        "* Naive Bayes Classifier : J'ai d'abord utilisé un classificateur Naive Bayes pour analyser des critiques de films provenant du corpus movie_reviews de NLTK. Le texte est prétraité et transformé en caractéristiques sous forme de mots (features), et le modèle est formé sur un sous-ensemble de données. Ensuite, la précision du modèle est évaluée sur un ensemble de test, et je teste également le classificateur sur un extrait de texte donné.\n",
        "* Pipeline Transformers (Hugging Face) : Ensuite, j'ai utilisé un pipeline de modèles pré-entraînés pour l'analyse de sentiment de Hugging Face. J'ai appliqué un modèle de classification de sentiment sur un extrait de texte, qui fournit directement une étiquette de sentiment (positif ou négatif) avec un score de confiance.\n",
        "* Réseau de Neurones (RNN avec PyTorch) : Enfin, j'ai implémenté un réseau de neurones récurrent (RNN) avec PyTorch pour l'analyse de sentiment. Le modèle utilise des embeddings, une couche LSTM et une couche linéaire pour prédire le sentiment d'une phrase. Le modèle est entraîné sur un jeu de données annotées automatiquement et évalué sur un ensemble de test. La perte et la précision sont calculées à chaque époque. Ce modèle est capable de prédire si une phrase est positive ou négative après l'entraînement."
      ],
      "metadata": {
        "id": "N0kyq12mPi-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Résumé"
      ],
      "metadata": {
        "id": "BchBUxL8VtAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sumy"
      ],
      "metadata": {
        "id": "uqWIHZpFSSUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAO-nzUgV9rW",
        "outputId": "15ce0564-3ef3-42c0-c704-57bfeede4a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=6a184d0ec5a8b858b49b7f4aef3ea7b7127a77d45a51151fb825a4e7eecb8ea6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0e94951766fc23a4333d4f4a90fd7eedd213bea4933dcd0dc97195c431cc5b9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer  # ou LexRankSummarizer, LuhnSummarizer, etc.\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "LANGUAGE = \"english\"  # ou \"french\"\n",
        "SENTENCES_COUNT = 5   # nombre de phrases du résumé\n",
        "\n",
        "text = cleaned_text[:10000]\n",
        "# Parser le texte\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
        "stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "# Choisir un algorithme de résumé\n",
        "summarizer = LsaSummarizer(stemmer)\n",
        "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "# Résumer\n",
        "summary = summarizer(parser.document, SENTENCES_COUNT)\n",
        "\n",
        "# Afficher\n",
        "print(\"Résumé :\\n\")\n",
        "for sentence in summary:\n",
        "    print(\"-\", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULNeScqtVu2D",
        "outputId": "6fc358da-37db-49cf-837e-d0822b77530f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résumé :\n",
            "\n",
            "- It was a fine, thick piece of wood, bulbous-headed, of the sort which is known as a “Penang lawyer.” Just under the head was a broad silver band nearly an inch across.\n",
            "- He had never said as much before, and I must admit that his words gave me keen pleasure, for I had often been piqued by his indifference to my admiration and to the attempts which I had made to give publicity to his methods.\n",
            "- So your grave, middle-aged family practitioner vanishes into thin air, my dear Watson, and there emerges a young fellow under thirty, amiable, unambitious, absent-minded, and the possessor of a favourite dog, which I should describe roughly as being larger than a terrier and smaller than a mastiff.”\n",
            "- I laughed incredulously as Sherlock Holmes leaned back in his settee and blew little wavering rings of smoke up to the ceiling.\n",
            "- It is my experience that it is only an amiable man in this world who receives testimonials, only an unambitious one who abandons a London career for the country, and only an absent-minded one who leaves his stick and not his visiting-card after waiting an hour in your room.”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bart"
      ],
      "metadata": {
        "id": "VKll0byhZ3QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialiser le pipeline de résumé avec un modèle pré-entraîné (ici BART pour le résumé)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Fonction pour découper un texte en morceaux\n",
        "def split_into_chunks(text, num_chunks=5):\n",
        "    chunk_length = len(text) // num_chunks\n",
        "    chunks = [text[i:i + chunk_length] for i in range(0, len(text), chunk_length)]\n",
        "\n",
        "    if len(chunks) > num_chunks:\n",
        "        chunks = chunks[:num_chunks]\n",
        "    elif len(chunks) < num_chunks:\n",
        "        last_chunk = chunks[-1] + text[len(chunks) * chunk_length:]\n",
        "        chunks[-1] = last_chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Découper le texte en 5 morceaux\n",
        "chunks = split_into_chunks(text, num_chunks=5)\n",
        "\n",
        "# Résumer chaque morceau\n",
        "for i, chunk in enumerate(chunks):\n",
        "    summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)\n",
        "    print(f\"\\nRésumé du Morceau {i+1} :\\n\", summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WasStgsYXl4",
        "outputId": "629f783c-6b12-45cd-91ab-a746c8490ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Résumé du Morceau 1 :\n",
            " The Hound of the Baskervilles, by A. Conan Doyle, published in 1884. The book is about the adventures of Sherlock Holmes and Dr. Watson. The story is told in 15 chapters.\n",
            "\n",
            "Résumé du Morceau 2 :\n",
            " Holmes asks Watson to reconstruct the identity of Dr. Mortimer. Watson says that Mortimer is a successful, elderly medical man. Holmes says that he is very much in Watson's debt for his work.\n",
            "\n",
            "Résumé du Morceau 3 :\n",
            " Watson had often been piqued by his indifference to his admiration and to his attempts to give publicity to his methods. He was proud, too, to think that he had so far mastered his system as to apply it in a way which earned his approval. He took the stick from my hands and examined it for a few minutes with his naked eyes.\n",
            "\n",
            "Résumé du Morceau 4 :\n",
            " ‘C.C.H.’ does stand for ‘Charing Cross Hospital’ ‘I can only think of the obvious conclusion that the man has. practised in town before going to the country,’ Sherlock Holmes said. ‘We believe there has been a change from a town hospital to a country practice’\n",
            "\n",
            "Résumé du Morceau 5 :\n",
            " Mortimer, James, M.R.C.S., 1882, Grimpen, Dartmoor, Devon. House-surgeon, from 1882 to 1884, at Charing Cross Hospital. Winner of Jackson prize for Comparative Pathology, with essay entitled ‘Is Disease a Reversion?’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les deux méthodes ont été utilisées pour résumer un texte :\n",
        "* Sumy : Un résumé est généré avec l'algorithme LSA (Latent Semantic Analysis), qui extrait les concepts clés du texte pour créer un résumé basé sur la sémantique. Le texte est analysé, tokenisé, puis résumé en 5 phrases.\n",
        "* Transformers (BART) : Le modèle pré-entraîné BART est utilisé pour générer un résumé en découpant le texte en morceaux. Chaque morceau est résumé de manière cohérente en ajustant la longueur du résumé.\n",
        "Ces deux techniques permettent de comparer un résumé statistique et un résumé généré par un modèle de deep learning."
      ],
      "metadata": {
        "id": "KGpuU8ZcTg2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse linguistique avec spacy"
      ],
      "metadata": {
        "id": "1sqQlC8MURN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Charger le modèle pré-entraîné en anglais\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Exemple de texte\n",
        "text = \"\"\"\n",
        "\n",
        "      Sherlock Holmes was born in London on January 6, 1854.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Traiter le texte avec le modèle spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extraire les entités nommées\n",
        "named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(\"Entités nommées extraites :\")\n",
        "for entity in named_entities:\n",
        "    print(f\"Texte : {entity[0]}, Type : {entity[1]}\")\n",
        "\n",
        "# Afficher les dépendances syntaxiques\n",
        "print(\"Dépendances syntaxiques :\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} (Tête : {token.head.text})\")\n",
        "\n",
        "# Visualisation des dépendances syntaxiques (relation entre les mots)\n",
        "displacy.serve(doc, style=\"dep\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L-Y-FQTi_HFV",
        "outputId": "517a07ab-0dd3-4d49-873e-77ff1033a847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entités nommées extraites :\n",
            "Texte : Holmes, Type : PERSON\n",
            "Texte : London, Type : GPE\n",
            "Texte : January 6, 1854, Type : DATE\n",
            "Dépendances syntaxiques :\n",
            "\n",
            "      \n",
            "       -> dep (Tête : Sherlock)\n",
            "Sherlock -> compound (Tête : Holmes)\n",
            "Holmes -> nsubjpass (Tête : born)\n",
            "was -> auxpass (Tête : born)\n",
            "born -> ROOT (Tête : born)\n",
            "in -> prep (Tête : born)\n",
            "London -> pobj (Tête : in)\n",
            "on -> prep (Tête : born)\n",
            "January -> pobj (Tête : on)\n",
            "6 -> nummod (Tête : January)\n",
            ", -> punct (Tête : January)\n",
            "1854 -> nummod (Tête : January)\n",
            ". -> punct (Tête : born)\n",
            "\n",
            "\n",
            " -> dep (Tête : .)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d2205eecb9c2418e90c8ce0113a202ef-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">\n",
              "      \n",
              "      </tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Sherlock</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Holmes</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">was</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">born</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">London</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">January</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">6,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">1854.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">\n",
              "\n",
              "</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-9\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d2205eecb9c2418e90c8ce0113a202ef-0-10\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d2205eecb9c2418e90c8ce0113a202ef-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1965.0,266.5 L1973.0,254.5 1957.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code utilise spaCy pour extraire les entités nommées (comme des personnes ou des lieux) et analyser les dépendances syntaxiques du texte. Il affiche également une visualisation graphique des relations entre les mots."
      ],
      "metadata": {
        "id": "eCQgD6YdUPcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traduction Automatique avec Encoder-Decoder"
      ],
      "metadata": {
        "id": "_D1siuJBKoxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Données\n",
        "english_sentences = [\n",
        "    \"Sherlock Holmes had, in a very remarkable degree, the power of detaching his mind at will.\",\n",
        "    \"For two hours the strange business in which we had been involved appeared to be forgotten, and he was entirely absorbed in the pictures of the modern Belgian masters.\",\n",
        "    \"He would talk of nothing but art, of which he had the crudest ideas, from our leaving the gallery until we found ourselves at the Northumberland Hotel.\"\n",
        "]\n",
        "\n",
        "french_sentences = [\n",
        "    \"[start] Sherlock Holmes possédait, à un degré tout à fait remarquable, le pouvoir de détacher son esprit à volonté. [end]\",\n",
        "    \"[start] Pendant deux heures, l'étrange affaire dans laquelle nous étions impliqués sembla être oubliée, et il fut entièrement absorbé par les tableaux des maîtres belges modernes. [end]\",\n",
        "    \"[start] Il ne parlait de rien d'autre que d'art, sujet dont il avait les idées les plus rudimentaires, depuis notre départ de la galerie jusqu'à ce que nous nous trouvions à l'Hôtel Northumberland. [end]\"\n",
        "]\n",
        "\n",
        "# Augmenter artificiellement les données (répétition)\n",
        "english_sentences = english_sentences * 5  # Répéter 5 fois\n",
        "french_sentences = french_sentences * 5\n",
        "\n",
        "# 2. Tokenisation\n",
        "def build_tokenizer(sentences, max_words=10000):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, filters='', lower=False)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "\n",
        "eng_tokenizer = build_tokenizer(english_sentences)\n",
        "fr_tokenizer = build_tokenizer(french_sentences)\n",
        "\n",
        "# Convertir en séquences\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "fr_sequences = fr_tokenizer.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Padding\n",
        "max_eng_len = max(len(seq) for seq in eng_sequences)\n",
        "max_fr_len = max(len(seq) for seq in fr_sequences)\n",
        "eng_sequences = tf.keras.preprocessing.sequence.pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
        "fr_sequences = tf.keras.preprocessing.sequence.pad_sequences(fr_sequences, maxlen=max_fr_len, padding='post')\n",
        "\n",
        "# Préparer le décodeur\n",
        "decoder_input_data = fr_sequences[:, :-1]\n",
        "decoder_target_data = fr_sequences[:, 1:]\n",
        "\n",
        "# Convertir les cibles en one-hot\n",
        "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "decoder_target_data = np.zeros((len(fr_sequences), max_fr_len-1, fr_vocab_size), dtype='float32')\n",
        "for i, seq in enumerate(fr_sequences[:, 1:]):\n",
        "    for t, word_idx in enumerate(seq):\n",
        "        if word_idx > 0:\n",
        "            decoder_target_data[i, t, word_idx] = 1.0\n",
        "\n",
        "# 3. Modèle Encoder-Decoder simplifié\n",
        "def build_model(eng_vocab_size, fr_vocab_size, max_eng_len, max_fr_len, units=32):\n",
        "    # Encodeur\n",
        "    encoder_inputs = layers.Input(shape=(max_eng_len,))\n",
        "    encoder_emb = layers.Embedding(eng_vocab_size, units)(encoder_inputs)\n",
        "    encoder_lstm = layers.LSTM(units, return_state=True)\n",
        "    _, state_h, state_c = encoder_lstm(encoder_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Décodeur\n",
        "    decoder_inputs = layers.Input(shape=(max_fr_len-1,))\n",
        "    decoder_emb = layers.Embedding(fr_vocab_size, units)(decoder_inputs)\n",
        "    decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state=encoder_states)\n",
        "    decoder_dense = layers.Dense(fr_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Modèle\n",
        "    model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "model = build_model(eng_vocab_size, fr_vocab_size, max_eng_len, max_fr_len)\n",
        "model.fit([eng_sequences, decoder_input_data], decoder_target_data, batch_size=2, epochs=100, verbose=1)\n",
        "\n",
        "# 4. Modèles d’inférence\n",
        "def build_inference_models(model, units=32):\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_emb = model.layers[2](encoder_inputs)\n",
        "    encoder_lstm = model.layers[4]\n",
        "    _, state_h, state_c = encoder_lstm(encoder_emb)\n",
        "    encoder_model = models.Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_state_input_h = layers.Input(shape=(units,))\n",
        "    decoder_state_input_c = layers.Input(shape=(units,))\n",
        "    decoder_emb = model.layers[3](decoder_inputs)\n",
        "    decoder_lstm = model.layers[5]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_emb, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "    decoder_dense = model.layers[6]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = models.Model(\n",
        "        [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n",
        "        [decoder_outputs] + [state_h, state_c]\n",
        "    )\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "encoder_model, decoder_model = build_inference_models(model)\n",
        "\n",
        "# 5. Traduction\n",
        "def translate_sentence(sentence, eng_tokenizer, fr_tokenizer, encoder_model, decoder_model, max_fr_len):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
        "    states = encoder_model.predict(seq, verbose=0)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = fr_tokenizer.word_index.get('[start]', 1)\n",
        "\n",
        "    output_tokens = []\n",
        "    for _ in range(max_fr_len):\n",
        "        output, h, c = decoder_model.predict([target_seq] + states, verbose=0)\n",
        "        idx = np.argmax(output[0, 0, :])\n",
        "        if idx == fr_tokenizer.word_index.get('[end]', 0) or idx == 0:\n",
        "            break\n",
        "        word = fr_tokenizer.index_word.get(idx, '')\n",
        "        if word:  # Ignorer les tokens vides\n",
        "            output_tokens.append(word)\n",
        "        target_seq[0, 0] = idx\n",
        "        states = [h, c]\n",
        "\n",
        "    return ' '.join(output_tokens)\n",
        "\n",
        "# 6. Tester\n",
        "print(\"Traductions:\")\n",
        "for sentence in english_sentences[:3]:  # Tester les 3 phrases originales\n",
        "    print(f\"EN: {sentence}\")\n",
        "    translation = translate_sentence(sentence, eng_tokenizer, fr_tokenizer, encoder_model, decoder_model, max_fr_len)\n",
        "    print(f\"FR: {translation}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItTIEyLAKJC5",
        "outputId": "660e6db3-123d-4226-830e-e57a84fbe82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 11s 60ms/step - accuracy: 0.0472 - loss: 3.2091\n",
            "Epoch 2/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 92ms/step - accuracy: 0.1581 - loss: 3.5567\n",
            "Epoch 3/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - accuracy: 0.1565 - loss: 3.2429\n",
            "Epoch 4/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - accuracy: 0.1584 - loss: 3.1081\n",
            "Epoch 5/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 100ms/step - accuracy: 0.1317 - loss: 3.1114\n",
            "Epoch 6/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - accuracy: 0.1100 - loss: 3.1735\n",
            "Epoch 7/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.1255 - loss: 2.9363\n",
            "Epoch 8/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1201 - loss: 3.0327\n",
            "Epoch 9/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.1559 - loss: 2.8703\n",
            "Epoch 10/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.1302 - loss: 2.7842\n",
            "Epoch 11/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.1217 - loss: 2.7035\n",
            "Epoch 12/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.1318 - loss: 2.8426\n",
            "Epoch 13/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.1431 - loss: 2.3332\n",
            "Epoch 14/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.1805 - loss: 2.4652\n",
            "Epoch 15/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.1825 - loss: 2.3942\n",
            "Epoch 16/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.2085 - loss: 2.1928\n",
            "Epoch 17/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.2338 - loss: 2.1965\n",
            "Epoch 18/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.1961 - loss: 2.0539\n",
            "Epoch 19/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.2167 - loss: 1.9623\n",
            "Epoch 20/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.2347 - loss: 1.9896\n",
            "Epoch 21/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 37ms/step - accuracy: 0.2382 - loss: 1.9666\n",
            "Epoch 22/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.2634 - loss: 2.0085\n",
            "Epoch 23/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.2835 - loss: 1.8879\n",
            "Epoch 24/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 36ms/step - accuracy: 0.2982 - loss: 1.8063\n",
            "Epoch 25/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step - accuracy: 0.3242 - loss: 1.8642\n",
            "Epoch 26/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.3417 - loss: 1.7267\n",
            "Epoch 27/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.3462 - loss: 1.5913\n",
            "Epoch 28/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.4151 - loss: 1.6845\n",
            "Epoch 29/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.4192 - loss: 1.6554\n",
            "Epoch 30/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.4088 - loss: 1.4964\n",
            "Epoch 31/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.4065 - loss: 1.4826\n",
            "Epoch 32/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.4476 - loss: 1.4926\n",
            "Epoch 33/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.4717 - loss: 1.5521\n",
            "Epoch 34/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.4871 - loss: 1.4278\n",
            "Epoch 35/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.4684 - loss: 1.4267\n",
            "Epoch 36/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.4422 - loss: 1.2656\n",
            "Epoch 37/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5423 - loss: 1.4418\n",
            "Epoch 38/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.4902 - loss: 1.3488\n",
            "Epoch 39/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5016 - loss: 1.3615\n",
            "Epoch 40/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5174 - loss: 1.2364\n",
            "Epoch 41/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5369 - loss: 1.3295\n",
            "Epoch 42/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5648 - loss: 1.3418\n",
            "Epoch 43/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5350 - loss: 1.1635\n",
            "Epoch 44/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5937 - loss: 1.2846\n",
            "Epoch 45/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5444 - loss: 1.2138\n",
            "Epoch 46/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5909 - loss: 1.1829\n",
            "Epoch 47/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.6457 - loss: 1.1894\n",
            "Epoch 48/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.6846 - loss: 1.2636\n",
            "Epoch 49/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.6385 - loss: 1.1282\n",
            "Epoch 50/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.6694 - loss: 1.1501\n",
            "Epoch 51/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.6664 - loss: 1.0792\n",
            "Epoch 52/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.6806 - loss: 1.0704\n",
            "Epoch 53/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.6706 - loss: 1.0496\n",
            "Epoch 54/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.6643 - loss: 1.1191\n",
            "Epoch 55/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.6815 - loss: 0.9591\n",
            "Epoch 56/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.6867 - loss: 0.9429\n",
            "Epoch 57/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7640 - loss: 1.0381\n",
            "Epoch 58/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7531 - loss: 1.0379\n",
            "Epoch 59/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7574 - loss: 0.9902\n",
            "Epoch 60/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7416 - loss: 0.9050\n",
            "Epoch 61/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7480 - loss: 0.8996\n",
            "Epoch 62/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7862 - loss: 0.9575\n",
            "Epoch 63/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.8068 - loss: 0.9663\n",
            "Epoch 64/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7441 - loss: 0.8803\n",
            "Epoch 65/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.8121 - loss: 0.9393\n",
            "Epoch 66/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 38ms/step - accuracy: 0.7661 - loss: 0.8662\n",
            "Epoch 67/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - accuracy: 0.7891 - loss: 0.8642\n",
            "Epoch 68/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.8226 - loss: 0.8905\n",
            "Epoch 69/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 39ms/step - accuracy: 0.7425 - loss: 0.7792\n",
            "Epoch 70/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - accuracy: 0.7437 - loss: 0.7424\n",
            "Epoch 71/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7867 - loss: 0.7872\n",
            "Epoch 72/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.7653 - loss: 0.7461\n",
            "Epoch 73/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7531 - loss: 0.7135\n",
            "Epoch 74/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7824 - loss: 0.7319\n",
            "Epoch 75/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.8315 - loss: 0.7908\n",
            "Epoch 76/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7890 - loss: 0.7154\n",
            "Epoch 77/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7559 - loss: 0.6680\n",
            "Epoch 78/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7961 - loss: 0.7069\n",
            "Epoch 79/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7513 - loss: 0.6358\n",
            "Epoch 80/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7813 - loss: 0.6643\n",
            "Epoch 81/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7636 - loss: 0.6141\n",
            "Epoch 82/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.8122 - loss: 0.6656\n",
            "Epoch 83/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7858 - loss: 0.6120\n",
            "Epoch 84/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7658 - loss: 0.5929\n",
            "Epoch 85/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.8182 - loss: 0.6207\n",
            "Epoch 86/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7718 - loss: 0.5625\n",
            "Epoch 87/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.8233 - loss: 0.6102\n",
            "Epoch 88/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7899 - loss: 0.5544\n",
            "Epoch 89/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.8129 - loss: 0.5739\n",
            "Epoch 90/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7574 - loss: 0.4955\n",
            "Epoch 91/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7965 - loss: 0.5272\n",
            "Epoch 92/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7971 - loss: 0.5001\n",
            "Epoch 93/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7804 - loss: 0.4914\n",
            "Epoch 94/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7961 - loss: 0.4849\n",
            "Epoch 95/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.8070 - loss: 0.4786\n",
            "Epoch 96/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7772 - loss: 0.4511\n",
            "Epoch 97/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.7622 - loss: 0.4301\n",
            "Epoch 98/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.8726 - loss: 0.5067\n",
            "Epoch 99/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.7635 - loss: 0.4089\n",
            "Epoch 100/100\n",
            "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.7928 - loss: 0.4188\n",
            "Traductions:\n",
            "EN: Sherlock Holmes had, in a very remarkable degree, the power of detaching his mind at will.\n",
            "FR: Sherlock Holmes possédait, à un degré tout à fait remarquable, le pouvoir de détacher son esprit à volonté.\n",
            "\n",
            "EN: For two hours the strange business in which we had been involved appeared to be forgotten, and he was entirely absorbed in the pictures of the modern Belgian masters.\n",
            "FR: Pendant deux heures, l'étrange affaire dans laquelle nous étions impliqués sembla être oubliée, et il fut entièrement absorbé par les tableaux des maîtres belges modernes.\n",
            "\n",
            "EN: He would talk of nothing but art, of which he had the crudest ideas, from our leaving the gallery until we found ourselves at the Northumberland Hotel.\n",
            "FR: Il ne parlait de rien d'autre que d'art, sujet dont il avait les idées les plus rudimentaires, depuis notre départ de la galerie jusqu'à ce que nous nous trouvions à l'Hôtel Northumberland.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il implémente un modèle de traduction automatique utilisant une architecture Encoder-Decoder avec LSTM. Il commence par préparer les données en anglais et en français, en tokenisant les phrases et en appliquant un padding pour uniformiser la longueur des séquences. Ensuite, un modèle Encoder-Decoder est construit : l'encodeur traite la phrase source (en anglais) et le décodeur génère la traduction (en français). Des modèles d'inférence sont utilisés pour traduire de nouvelles phrases en temps réel. Enfin, le modèle est testé avec quelques phrases en anglais, qui sont traduites en français."
      ],
      "metadata": {
        "id": "gZE6lKE_U2JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generer le texte"
      ],
      "metadata": {
        "id": "hbY18oE-eQVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Charger le modèle et le tokenizer\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "# Ton extrait de roman\n",
        "extrait = (\n",
        "   \"\"\" It is murder, Watson—refined, cold-blooded, deliberate murder.\n",
        "      Do not ask me for particulars. My nets are closing upon him, even\n",
        "      as his are upon Sir Henry, and with your help he is already\n",
        "      almost at my mercy.\"\"\"\n",
        ")\n",
        "\n",
        "# Préparer l'entrée pour le modèle\n",
        "inputs = tokenizer.encode(extrait, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Générer la suite du texte\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Afficher le texte généré\n",
        "texte_genere = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(texte_genere)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FwF_qXMbA7T",
        "outputId": "59bc2061-05e1-44cc-9e98-e89f3b6a622a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-086b210c2daf>\", line 1, in <cell line: 0>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It is murder, Watson—refined, cold-blooded, deliberate murder.\n",
            "      Do not ask me for particulars. My nets are closing upon him, even\n",
            "      as his are upon Sir Henry, and with your help he is already\n",
            "      almost at my mercy. I shall not be so easily caught, however, and\n",
            "      I will not be so easily killed.\n",
            "\n",
            "He paused, looking at me with an unyielding eye, while I stood silent\n",
            "and motionless, my head bent and my eyes fixed on the floor.\n",
            "\n",
            "\"I will not be so easily caught, however, and I will not be so easily\n",
            "killed,\" he repeated.\n",
            "\n",
            "He paused, looking at me with an unyielding eye, while I stood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le texte orginal:\n",
        "It is murder, Watson—refined, cold-blooded, deliberate murder.\n",
        "Do not ask me for particulars. My nets are closing upon him, even\n",
        "      as his are upon Sir Henry, and with your help he is already\n",
        "      almost at my mercy. There is but one danger which can threaten\n",
        "      us. It is that he should strike before we are ready to do so.\n",
        "      Another day—two at the most—and I have my case complete, but\n",
        "      until then guard your charge as closely as ever a fond mother\n",
        "      watched her ailing child. Your mission today has justified\n",
        "      itself, and yet I could almost wish that you had not left his\n",
        "      side. Hark!"
      ],
      "metadata": {
        "id": "7C4NyfrAc5sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est un modèle pré-entraîné \"SmolLM2-1.7B-Instruct\" de HuggingFace et son tokenizer. Il prend un extrait de texte de roman et le prépare pour le modèle en l'encodant. Ensuite, il génère une suite du texte en utilisant des paramètres de génération tels que la température et le top-p pour contrôler la diversité de la sortie.  Cela permet de créer une continuation fluide et créative de l'extrait donné.\n",
        "\n",
        "Le modèle a généré du texte qui suit bien le début de l'extrait, mais il a tendance à répéter certaines phrases. Ce comportement peut être causé par les paramètres de génération, notamment la température et le top_p, qui favorisent des répétitions. De plus, le texte généré s'éloigne un peu du texte original, ce qui montre que le modèle prend des libertés créatives."
      ],
      "metadata": {
        "id": "Zz8GguISVQN3"
      }
    }
  ]
}